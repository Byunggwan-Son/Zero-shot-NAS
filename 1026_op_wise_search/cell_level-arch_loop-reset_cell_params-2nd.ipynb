{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902bb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 05:59:35.150653: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "from custom_models import get_cell_based_tiny_net\n",
    "from custom_search_cells import NAS201SearchCell as SearchCell\n",
    "from xautodl.models.cell_searchs.genotypes import Structure\n",
    "\n",
    "# NB201\n",
    "from nas_201_api import NASBench201API as API\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792763c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66857\n",
      "Namespace(arch_nas_dataset=None, channel=16, config_path='./MY.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=66857, save_dir='./cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='../cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space_name\", type=str, default='nas-bench-201', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./MY.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "# log\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200', help=\"Folder to save checkpoints and log.\")\n",
    "# parser.add_argument(\"--arch_nas_dataset\", type=str, default='../NAS-Bench-201-v1_1-096897.pth', help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--arch_nas_dataset\", type=str, default=None, help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "    \n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcd1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : None\n",
      "channel          : 16\n",
      "config_path      : ./MY.config\n",
      "data_path        : ../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 66857\n",
      "save_dir         : ./cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "Python  Version  : 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0]\n",
      "Pillow  Version  : 9.0.1\n",
      "PyTorch Version  : 1.12.0\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9daa5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./MY.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=50, current::epoch=0, iter=0.00, type=cosine, T-max=50, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "[2022-11-03 05:59:37] create API = None done\n",
      "=> do not find the last-info file : cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/seed-66857-last-info.pth\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.test_batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": bool(xargs.track_running_stats),\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "# if xargs.arch_nas_dataset is None:\n",
    "api = None\n",
    "# else:\n",
    "#     api = API(xargs.arch_nas_dataset)\n",
    "logger.log(\"{:} create API = {:} done\".format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "if last_info.exists():  # automatically resume from previous checkpoint\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start\".format(last_info)\n",
    "    )\n",
    "    last_info = torch.load(last_info)\n",
    "    start_epoch = last_info[\"epoch\"]\n",
    "    checkpoint = torch.load(last_info[\"last_checkpoint\"])\n",
    "    genotypes = checkpoint[\"genotypes\"]\n",
    "    valid_accuracies = checkpoint[\"valid_accuracies\"]\n",
    "    search_model.load_state_dict(checkpoint[\"search_model\"])\n",
    "    w_scheduler.load_state_dict(checkpoint[\"w_scheduler\"])\n",
    "    w_optimizer.load_state_dict(checkpoint[\"w_optimizer\"])\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(\n",
    "            last_info, start_epoch\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ba0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_confidence_robustness_metrics(network, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        # accuracy\n",
    "        network.train()\n",
    "        _, logits = network(inputs)\n",
    "        val_top1, val_top5 = obtain_accuracy(logits.data, targets.data, topk=(1, 5))\n",
    "        acc = val_top1\n",
    "        \n",
    "        # confidence\n",
    "        prob = torch.nn.functional.softmax(logits, dim=1)\n",
    "        one_hot_idx = torch.nn.functional.one_hot(targets)\n",
    "        confidence = (prob[one_hot_idx==1].sum()) / inputs.size(0) * 100 # in percent\n",
    "        \n",
    "        # sensitivity\n",
    "        _, noisy_logits = network(inputs + torch.randn_like(inputs)*0.1)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        sensitivity = kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        \n",
    "        # robustness\n",
    "        original_weights = deepcopy(network.state_dict())\n",
    "        for m in network.modules():\n",
    "            if isinstance(m, SearchCell):\n",
    "                for p in m.parameters():\n",
    "                    p.add_(torch.randn_like(p) * p.std()*0.3)\n",
    "            \n",
    "        _, noisy_logits = network(inputs)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        robustness = -kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        network.load_state_dict(original_weights)\n",
    "                \n",
    "        return acc.item(), confidence.item(), sensitivity.item(), robustness.item()\n",
    "    \n",
    "def step_sim_metric(network, criterion, inputs, targets):\n",
    "    inputs, targets = inputs[:64], targets[:64] # smaller batches\n",
    "    original_dict = deepcopy(network.state_dict())\n",
    "    optim_large_step = torch.optim.SGD(network.parameters(), lr=0.025)\n",
    "    \n",
    "    # single large step\n",
    "    network.train()\n",
    "    optim_large_step.zero_grad()\n",
    "    _, logits = network(inputs)\n",
    "    base_loss = criterion(logits, targets)\n",
    "    base_loss.backward()\n",
    "    optim_large_step.step()\n",
    "    large_step_dict = deepcopy(network.state_dict())\n",
    "    \n",
    "    # multiple small steps\n",
    "    network.load_state_dict(original_dict)\n",
    "    optim_small_step = torch.optim.SGD(network.parameters(), lr=0.025/3)\n",
    "    for i in range(3):\n",
    "        optim_small_step.zero_grad()\n",
    "        _, logits = network(inputs)\n",
    "        base_loss = criterion(logits, targets)\n",
    "        base_loss.backward()\n",
    "        optim_small_step.step()\n",
    "    small_step_dict = deepcopy(network.state_dict())\n",
    "    scores = []\n",
    "    for key in large_step_dict.keys():\n",
    "        if ('weight' in key) and (original_dict[key].dim()==4):\n",
    "            if (original_dict[key] != large_step_dict[key]).sum():\n",
    "                large_step = large_step_dict[key] - original_dict[key]\n",
    "                small_step = small_step_dict[key] - original_dict[key]\n",
    "                co, ci, kh, kw = large_step.size()\n",
    "                large_step = large_step.view(co, -1)\n",
    "                small_step = small_step.view(co, -1)\n",
    "                score = torch.nn.functional.cosine_similarity(large_step, small_step, dim=1)\n",
    "                score = score.mean().item() * 100 # in percent\n",
    "                scores.append(score)\n",
    "    if len(scores)==0:\n",
    "        step_sim = 100\n",
    "        raise RuntimeError\n",
    "    else:\n",
    "        step_sim = np.mean(scores)\n",
    "    \n",
    "    # resume\n",
    "    network.load_state_dict(original_dict)\n",
    "            \n",
    "    return step_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67cdab22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of nodes:60\n",
      "target_node:1 4\n",
      "target_node:2 24\n",
      "target_node:3 124\n",
      "\n",
      "\n",
      " Searching with a cell #0\n",
      "*Train* [2022-11-03 05:59:41] Ep:0 [000/391] Time 2.48 (2.48) Data 0.13 (0.13) Base [Loss 2.285 (2.285)  Prec@1 12.50 (12.50) Prec@5 54.69 (54.69)]\n",
      "*Train* [2022-11-03 06:00:00] Ep:0 [200/391] Time 0.18 (0.11) Data 0.00 (0.00) Base [Loss 1.688 (1.825)  Prec@1 31.25 (30.60) Prec@5 87.50 (84.78)]\n",
      "*Train* [2022-11-03 06:00:18] Ep:0 [390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 1.514 (1.697)  Prec@1 42.50 (35.99) Prec@5 95.00 (87.80)]\n",
      "Ep:0 ends : loss=1.70, accuracy@1=35.99%, accuracy@5=87.80%\n",
      "*Train* [2022-11-03 06:00:18] Ep:1 [000/391] Time 0.23 (0.23) Data 0.15 (0.15) Base [Loss 2.227 (2.227)  Prec@1 28.12 (28.12) Prec@5 73.44 (73.44)]\n",
      "*Train* [2022-11-03 06:00:42] Ep:1 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 1.458 (1.489)  Prec@1 46.88 (45.75) Prec@5 92.19 (91.78)]\n",
      "*Train* [2022-11-03 06:01:04] Ep:1 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 1.355 (1.415)  Prec@1 55.00 (48.32) Prec@5 87.50 (92.79)]\n",
      "Ep:1 ends : loss=1.42, accuracy@1=48.32%, accuracy@5=92.79%\n",
      "*Train* [2022-11-03 06:01:04] Ep:2 [000/391] Time 0.24 (0.24) Data 0.14 (0.14) Base [Loss 2.862 (2.862)  Prec@1 17.19 (17.19) Prec@5 60.94 (60.94)]\n",
      "*Train* [2022-11-03 06:01:27] Ep:2 [200/391] Time 0.21 (0.12) Data 0.00 (0.00) Base [Loss 1.071 (1.317)  Prec@1 62.50 (52.60) Prec@5 96.88 (93.44)]\n",
      "*Train* [2022-11-03 06:01:47] Ep:2 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 1.165 (1.262)  Prec@1 60.00 (54.68) Prec@5 100.00 (94.30)]\n",
      "Ep:2 ends : loss=1.26, accuracy@1=54.68%, accuracy@5=94.30%\n",
      "*Train* [2022-11-03 06:01:47] Ep:3 [000/391] Time 0.31 (0.31) Data 0.16 (0.16) Base [Loss 2.743 (2.743)  Prec@1 20.31 (20.31) Prec@5 64.06 (64.06)]\n",
      "*Train* [2022-11-03 06:02:08] Ep:3 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 1.442 (1.537)  Prec@1 51.56 (44.08) Prec@5 87.50 (90.55)]\n",
      "*Train* [2022-11-03 06:02:26] Ep:3 [390/391] Time 0.13 (0.10) Data 0.00 (0.00) Base [Loss 1.235 (1.427)  Prec@1 50.00 (48.44) Prec@5 92.50 (92.40)]\n",
      "Ep:3 ends : loss=1.43, accuracy@1=48.44%, accuracy@5=92.40%\n",
      "*Train* [2022-11-03 06:02:27] Ep:4 [000/391] Time 0.25 (0.25) Data 0.17 (0.17) Base [Loss 1.560 (1.560)  Prec@1 42.19 (42.19) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 06:02:48] Ep:4 [200/391] Time 0.19 (0.11) Data 0.00 (0.00) Base [Loss 1.090 (1.190)  Prec@1 56.25 (57.87) Prec@5 95.31 (95.36)]\n",
      "*Train* [2022-11-03 06:03:13] Ep:4 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 1.450 (1.161)  Prec@1 55.00 (58.72) Prec@5 90.00 (95.62)]\n",
      "Ep:4 ends : loss=1.16, accuracy@1=58.72%, accuracy@5=95.62%\n",
      "*Train* [2022-11-03 06:03:13] Ep:5 [000/391] Time 0.33 (0.33) Data 0.19 (0.19) Base [Loss 1.992 (1.992)  Prec@1 35.94 (35.94) Prec@5 82.81 (82.81)]\n",
      "*Train* [2022-11-03 06:03:31] Ep:5 [200/391] Time 0.07 (0.09) Data 0.00 (0.00) Base [Loss 0.912 (1.118)  Prec@1 68.75 (60.11) Prec@5 98.44 (96.09)]\n",
      "*Train* [2022-11-03 06:03:54] Ep:5 [390/391] Time 0.20 (0.10) Data 0.00 (0.00) Base [Loss 0.958 (1.079)  Prec@1 67.50 (61.60) Prec@5 95.00 (96.28)]\n",
      "Ep:5 ends : loss=1.08, accuracy@1=61.60%, accuracy@5=96.28%\n",
      "*Train* [2022-11-03 06:03:54] Ep:6 [000/391] Time 0.30 (0.30) Data 0.19 (0.19) Base [Loss 1.662 (1.662)  Prec@1 35.94 (35.94) Prec@5 85.94 (85.94)]\n",
      "*Train* [2022-11-03 06:04:10] Ep:6 [200/391] Time 0.13 (0.08) Data 0.00 (0.00) Base [Loss 0.931 (1.064)  Prec@1 67.19 (62.35) Prec@5 98.44 (96.14)]\n",
      "*Train* [2022-11-03 06:04:28] Ep:6 [390/391] Time 0.06 (0.09) Data 0.00 (0.00) Base [Loss 1.046 (1.037)  Prec@1 60.00 (63.30) Prec@5 97.50 (96.53)]\n",
      "Ep:6 ends : loss=1.04, accuracy@1=63.30%, accuracy@5=96.53%\n",
      "*Train* [2022-11-03 06:04:28] Ep:7 [000/391] Time 0.30 (0.30) Data 0.17 (0.17) Base [Loss 1.472 (1.472)  Prec@1 54.69 (54.69) Prec@5 87.50 (87.50)]\n",
      "*Train* [2022-11-03 06:04:47] Ep:7 [200/391] Time 0.13 (0.09) Data 0.00 (0.00) Base [Loss 0.995 (1.004)  Prec@1 68.75 (64.26) Prec@5 100.00 (96.88)]\n",
      "*Train* [2022-11-03 06:05:11] Ep:7 [390/391] Time 0.21 (0.11) Data 0.00 (0.00) Base [Loss 0.938 (0.978)  Prec@1 67.50 (65.37) Prec@5 97.50 (96.88)]\n",
      "Ep:7 ends : loss=0.98, accuracy@1=65.37%, accuracy@5=96.88%\n",
      "*Train* [2022-11-03 06:05:11] Ep:8 [000/391] Time 0.26 (0.26) Data 0.18 (0.18) Base [Loss 1.475 (1.475)  Prec@1 46.88 (46.88) Prec@5 89.06 (89.06)]\n",
      "*Train* [2022-11-03 06:05:34] Ep:8 [200/391] Time 0.20 (0.11) Data 0.00 (0.00) Base [Loss 0.821 (0.977)  Prec@1 71.88 (65.13) Prec@5 95.31 (96.77)]\n",
      "*Train* [2022-11-03 06:05:54] Ep:8 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 1.014 (0.950)  Prec@1 60.00 (66.34) Prec@5 100.00 (96.92)]\n",
      "Ep:8 ends : loss=0.95, accuracy@1=66.34%, accuracy@5=96.92%\n",
      "*Train* [2022-11-03 06:05:54] Ep:9 [000/391] Time 0.30 (0.30) Data 0.17 (0.17) Base [Loss 1.186 (1.186)  Prec@1 62.50 (62.50) Prec@5 93.75 (93.75)]\n",
      "*Train* [2022-11-03 06:06:13] Ep:9 [200/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 0.872 (0.933)  Prec@1 60.94 (66.90) Prec@5 98.44 (97.43)]\n",
      "*Train* [2022-11-03 06:06:30] Ep:9 [390/391] Time 0.13 (0.09) Data 0.00 (0.00) Base [Loss 0.947 (0.916)  Prec@1 62.50 (67.84) Prec@5 100.00 (97.46)]\n",
      "Ep:9 ends : loss=0.92, accuracy@1=67.84%, accuracy@5=97.46%\n",
      "Found best op for target cell:0\n",
      ": Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|none~0|nor_conv_3x3~1|none~2|) with accuracy=67.38%, confidence=56.156%, sensitivity=0.059, robustness=-0.013, step_sim=99.822\n",
      "\n",
      "\n",
      " Searching with a cell #1\n",
      "*Train* [2022-11-03 06:09:32] Ep:0 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 1.208 (1.208)  Prec@1 54.69 (54.69) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:09:49] Ep:0 [200/391] Time 0.13 (0.09) Data 0.00 (0.00) Base [Loss 0.808 (0.982)  Prec@1 67.19 (65.21) Prec@5 96.88 (96.86)]\n",
      "*Train* [2022-11-03 06:10:05] Ep:0 [390/391] Time 0.08 (0.08) Data 0.00 (0.00) Base [Loss 0.801 (0.949)  Prec@1 72.50 (66.69) Prec@5 97.50 (97.03)]\n",
      "Ep:0 ends : loss=0.95, accuracy@1=66.69%, accuracy@5=97.03%\n",
      "*Train* [2022-11-03 06:10:05] Ep:1 [000/391] Time 0.31 (0.31) Data 0.15 (0.15) Base [Loss 2.288 (2.288)  Prec@1 32.81 (32.81) Prec@5 81.25 (81.25)]\n",
      "*Train* [2022-11-03 06:10:21] Ep:1 [200/391] Time 0.13 (0.08) Data 0.00 (0.00) Base [Loss 0.884 (0.913)  Prec@1 62.50 (68.07) Prec@5 98.44 (97.22)]\n",
      "*Train* [2022-11-03 06:10:40] Ep:1 [390/391] Time 0.13 (0.09) Data 0.00 (0.00) Base [Loss 0.954 (0.889)  Prec@1 57.50 (69.01) Prec@5 100.00 (97.40)]\n",
      "Ep:1 ends : loss=0.89, accuracy@1=69.01%, accuracy@5=97.40%\n",
      "*Train* [2022-11-03 06:10:41] Ep:2 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 2.325 (2.325)  Prec@1 26.56 (26.56) Prec@5 70.31 (70.31)]\n",
      "*Train* [2022-11-03 06:10:58] Ep:2 [200/391] Time 0.13 (0.09) Data 0.00 (0.00) Base [Loss 0.938 (0.888)  Prec@1 68.75 (68.87) Prec@5 93.75 (97.50)]\n",
      "*Train* [2022-11-03 06:11:16] Ep:2 [390/391] Time 0.07 (0.09) Data 0.00 (0.00) Base [Loss 1.001 (0.865)  Prec@1 65.00 (69.60) Prec@5 97.50 (97.57)]\n",
      "Ep:2 ends : loss=0.86, accuracy@1=69.60%, accuracy@5=97.57%\n",
      "*Train* [2022-11-03 06:11:17] Ep:3 [000/391] Time 0.31 (0.31) Data 0.17 (0.17) Base [Loss 2.273 (2.273)  Prec@1 31.25 (31.25) Prec@5 87.50 (87.50)]\n",
      "*Train* [2022-11-03 06:11:40] Ep:3 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 1.006 (0.872)  Prec@1 68.75 (69.52) Prec@5 96.88 (97.83)]\n",
      "*Train* [2022-11-03 06:11:59] Ep:3 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.529 (0.839)  Prec@1 75.00 (70.74) Prec@5 100.00 (97.88)]\n",
      "Ep:3 ends : loss=0.84, accuracy@1=70.74%, accuracy@5=97.88%\n",
      "*Train* [2022-11-03 06:12:00] Ep:4 [000/391] Time 0.25 (0.25) Data 0.17 (0.17) Base [Loss 1.478 (1.478)  Prec@1 51.56 (51.56) Prec@5 90.62 (90.62)]\n",
      "*Train* [2022-11-03 06:12:19] Ep:4 [200/391] Time 0.14 (0.10) Data 0.00 (0.00) Base [Loss 0.927 (0.841)  Prec@1 67.19 (70.37) Prec@5 93.75 (97.51)]\n",
      "*Train* [2022-11-03 06:12:39] Ep:4 [390/391] Time 0.12 (0.10) Data 0.00 (0.00) Base [Loss 0.673 (0.821)  Prec@1 80.00 (71.08) Prec@5 100.00 (97.81)]\n",
      "Ep:4 ends : loss=0.82, accuracy@1=71.08%, accuracy@5=97.81%\n",
      "*Train* [2022-11-03 06:12:39] Ep:5 [000/391] Time 0.28 (0.28) Data 0.20 (0.20) Base [Loss 0.941 (0.941)  Prec@1 67.19 (67.19) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:13:04] Ep:5 [200/391] Time 0.20 (0.13) Data 0.00 (0.00) Base [Loss 0.649 (0.778)  Prec@1 76.56 (72.73) Prec@5 100.00 (98.03)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 06:13:22] Ep:5 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.702 (0.779)  Prec@1 70.00 (72.80) Prec@5 100.00 (98.08)]\n",
      "Ep:5 ends : loss=0.78, accuracy@1=72.80%, accuracy@5=98.08%\n",
      "*Train* [2022-11-03 06:13:23] Ep:6 [000/391] Time 0.26 (0.26) Data 0.18 (0.18) Base [Loss 1.590 (1.590)  Prec@1 43.75 (43.75) Prec@5 90.62 (90.62)]\n",
      "*Train* [2022-11-03 06:13:39] Ep:6 [200/391] Time 0.13 (0.08) Data 0.00 (0.00) Base [Loss 0.820 (0.800)  Prec@1 70.31 (71.77) Prec@5 98.44 (98.20)]\n",
      "*Train* [2022-11-03 06:14:04] Ep:6 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.744 (0.784)  Prec@1 77.50 (72.46) Prec@5 100.00 (98.22)]\n",
      "Ep:6 ends : loss=0.78, accuracy@1=72.46%, accuracy@5=98.22%\n",
      "*Train* [2022-11-03 06:14:04] Ep:7 [000/391] Time 0.33 (0.33) Data 0.18 (0.18) Base [Loss 3.596 (3.596)  Prec@1 15.62 (15.62) Prec@5 57.81 (57.81)]\n",
      "*Train* [2022-11-03 06:14:23] Ep:7 [200/391] Time 0.15 (0.10) Data 0.00 (0.00) Base [Loss 0.895 (0.846)  Prec@1 62.50 (70.65) Prec@5 100.00 (97.29)]\n",
      "*Train* [2022-11-03 06:14:41] Ep:7 [390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 0.599 (0.808)  Prec@1 80.00 (71.91) Prec@5 97.50 (97.70)]\n",
      "Ep:7 ends : loss=0.81, accuracy@1=71.91%, accuracy@5=97.70%\n",
      "*Train* [2022-11-03 06:14:41] Ep:8 [000/391] Time 0.28 (0.28) Data 0.19 (0.19) Base [Loss 0.939 (0.939)  Prec@1 68.75 (68.75) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:15:03] Ep:8 [200/391] Time 0.15 (0.11) Data 0.00 (0.00) Base [Loss 0.830 (0.743)  Prec@1 73.44 (74.28) Prec@5 100.00 (98.17)]\n",
      "*Train* [2022-11-03 06:15:21] Ep:8 [390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 0.725 (0.741)  Prec@1 72.50 (74.47) Prec@5 100.00 (98.18)]\n",
      "Ep:8 ends : loss=0.74, accuracy@1=74.47%, accuracy@5=98.18%\n",
      "*Train* [2022-11-03 06:15:22] Ep:9 [000/391] Time 0.24 (0.24) Data 0.15 (0.15) Base [Loss 0.777 (0.777)  Prec@1 71.88 (71.88) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 06:15:39] Ep:9 [200/391] Time 0.14 (0.09) Data 0.00 (0.00) Base [Loss 0.684 (0.761)  Prec@1 78.12 (73.66) Prec@5 96.88 (98.17)]\n",
      "*Train* [2022-11-03 06:15:56] Ep:9 [390/391] Time 0.12 (0.09) Data 0.00 (0.00) Base [Loss 0.828 (0.741)  Prec@1 70.00 (74.22) Prec@5 97.50 (98.24)]\n",
      "Ep:9 ends : loss=0.74, accuracy@1=74.22%, accuracy@5=98.24%\n",
      "Found best op for target cell:1\n",
      ": Structure(4 nodes with |nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_1x1~1|none~2|) with accuracy=66.02%, confidence=54.070%, sensitivity=0.102, robustness=-0.030, step_sim=99.591\n",
      "\n",
      "\n",
      " Searching with a cell #2\n",
      "*Train* [2022-11-03 06:18:28] Ep:0 [000/391] Time 0.27 (0.27) Data 0.19 (0.19) Base [Loss 1.740 (1.740)  Prec@1 42.19 (42.19) Prec@5 87.50 (87.50)]\n",
      "*Train* [2022-11-03 06:18:46] Ep:0 [200/391] Time 0.13 (0.09) Data 0.00 (0.00) Base [Loss 0.671 (0.777)  Prec@1 78.12 (73.09) Prec@5 100.00 (98.12)]\n",
      "*Train* [2022-11-03 06:19:07] Ep:0 [390/391] Time 0.13 (0.10) Data 0.00 (0.00) Base [Loss 0.496 (0.750)  Prec@1 85.00 (73.90) Prec@5 100.00 (98.24)]\n",
      "Ep:0 ends : loss=0.75, accuracy@1=73.90%, accuracy@5=98.24%\n",
      "*Train* [2022-11-03 06:19:08] Ep:1 [000/391] Time 0.32 (0.32) Data 0.17 (0.17) Base [Loss 1.282 (1.282)  Prec@1 57.81 (57.81) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 06:19:31] Ep:1 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.594 (0.737)  Prec@1 81.25 (74.61) Prec@5 100.00 (98.39)]\n",
      "*Train* [2022-11-03 06:19:51] Ep:1 [390/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.664 (0.729)  Prec@1 85.00 (74.68) Prec@5 100.00 (98.48)]\n",
      "Ep:1 ends : loss=0.73, accuracy@1=74.68%, accuracy@5=98.48%\n",
      "*Train* [2022-11-03 06:19:51] Ep:2 [000/391] Time 0.30 (0.30) Data 0.16 (0.16) Base [Loss 0.889 (0.889)  Prec@1 73.44 (73.44) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:20:17] Ep:2 [200/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.635 (0.699)  Prec@1 73.44 (75.68) Prec@5 100.00 (98.62)]\n",
      "*Train* [2022-11-03 06:20:38] Ep:2 [390/391] Time 0.22 (0.12) Data 0.00 (0.00) Base [Loss 0.759 (0.695)  Prec@1 72.50 (75.87) Prec@5 100.00 (98.56)]\n",
      "Ep:2 ends : loss=0.69, accuracy@1=75.87%, accuracy@5=98.56%\n",
      "*Train* [2022-11-03 06:20:38] Ep:3 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 2.272 (2.272)  Prec@1 32.81 (32.81) Prec@5 73.44 (73.44)]\n",
      "*Train* [2022-11-03 06:20:57] Ep:3 [200/391] Time 0.13 (0.10) Data 0.00 (0.00) Base [Loss 0.738 (0.753)  Prec@1 78.12 (73.49) Prec@5 98.44 (98.33)]\n",
      "*Train* [2022-11-03 06:21:18] Ep:3 [390/391] Time 0.08 (0.10) Data 0.00 (0.00) Base [Loss 0.805 (0.729)  Prec@1 80.00 (74.76) Prec@5 95.00 (98.35)]\n",
      "Ep:3 ends : loss=0.73, accuracy@1=74.76%, accuracy@5=98.35%\n",
      "*Train* [2022-11-03 06:21:18] Ep:4 [000/391] Time 0.36 (0.36) Data 0.19 (0.19) Base [Loss 2.092 (2.092)  Prec@1 45.31 (45.31) Prec@5 76.56 (76.56)]\n",
      "*Train* [2022-11-03 06:21:46] Ep:4 [200/391] Time 0.13 (0.14) Data 0.00 (0.00) Base [Loss 0.735 (0.771)  Prec@1 76.56 (73.51) Prec@5 98.44 (97.94)]\n",
      "*Train* [2022-11-03 06:22:05] Ep:4 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.431 (0.733)  Prec@1 77.50 (74.55) Prec@5 100.00 (98.19)]\n",
      "Ep:4 ends : loss=0.73, accuracy@1=74.55%, accuracy@5=98.19%\n",
      "*Train* [2022-11-03 06:22:06] Ep:5 [000/391] Time 0.36 (0.36) Data 0.18 (0.18) Base [Loss 2.107 (2.107)  Prec@1 37.50 (37.50) Prec@5 82.81 (82.81)]\n",
      "*Train* [2022-11-03 06:22:27] Ep:5 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.693 (0.729)  Prec@1 76.56 (74.93) Prec@5 98.44 (98.41)]\n",
      "*Train* [2022-11-03 06:22:47] Ep:5 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.831 (0.703)  Prec@1 72.50 (75.61) Prec@5 97.50 (98.52)]\n",
      "Ep:5 ends : loss=0.70, accuracy@1=75.61%, accuracy@5=98.52%\n",
      "*Train* [2022-11-03 06:22:47] Ep:6 [000/391] Time 0.27 (0.27) Data 0.15 (0.15) Base [Loss 0.863 (0.863)  Prec@1 71.88 (71.88) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:23:10] Ep:6 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.723 (0.716)  Prec@1 78.12 (74.86) Prec@5 95.31 (98.39)]\n",
      "*Train* [2022-11-03 06:23:31] Ep:6 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.586 (0.700)  Prec@1 75.00 (75.55) Prec@5 100.00 (98.44)]\n",
      "Ep:6 ends : loss=0.70, accuracy@1=75.55%, accuracy@5=98.44%\n",
      "*Train* [2022-11-03 06:23:32] Ep:7 [000/391] Time 0.25 (0.25) Data 0.16 (0.16) Base [Loss 1.154 (1.154)  Prec@1 59.38 (59.38) Prec@5 93.75 (93.75)]\n",
      "*Train* [2022-11-03 06:23:56] Ep:7 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.933 (0.708)  Prec@1 65.62 (75.29) Prec@5 96.88 (98.41)]\n",
      "*Train* [2022-11-03 06:24:18] Ep:7 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.770 (0.694)  Prec@1 72.50 (75.90) Prec@5 100.00 (98.49)]\n",
      "Ep:7 ends : loss=0.69, accuracy@1=75.90%, accuracy@5=98.49%\n",
      "*Train* [2022-11-03 06:24:18] Ep:8 [000/391] Time 0.33 (0.33) Data 0.17 (0.17) Base [Loss 0.874 (0.874)  Prec@1 67.19 (67.19) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:24:43] Ep:8 [200/391] Time 0.12 (0.13) Data 0.00 (0.00) Base [Loss 0.583 (0.671)  Prec@1 81.25 (76.73) Prec@5 100.00 (98.65)]\n",
      "*Train* [2022-11-03 06:25:06] Ep:8 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.847 (0.667)  Prec@1 72.50 (76.93) Prec@5 92.50 (98.61)]\n",
      "Ep:8 ends : loss=0.67, accuracy@1=76.93%, accuracy@5=98.61%\n",
      "*Train* [2022-11-03 06:25:06] Ep:9 [000/391] Time 0.28 (0.28) Data 0.19 (0.19) Base [Loss 0.862 (0.862)  Prec@1 76.56 (76.56) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 06:25:26] Ep:9 [200/391] Time 0.08 (0.10) Data 0.00 (0.00) Base [Loss 0.757 (0.656)  Prec@1 78.12 (77.37) Prec@5 100.00 (98.68)]\n",
      "*Train* [2022-11-03 06:25:46] Ep:9 [390/391] Time 0.12 (0.10) Data 0.00 (0.00) Base [Loss 0.535 (0.655)  Prec@1 80.00 (77.45) Prec@5 100.00 (98.68)]\n",
      "Ep:9 ends : loss=0.65, accuracy@1=77.45%, accuracy@5=98.68%\n",
      "Found best op for target cell:2\n",
      ": Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|none~1|+|skip_connect~0|none~1|none~2|) with accuracy=69.14%, confidence=58.916%, sensitivity=0.117, robustness=-0.041, step_sim=99.664\n",
      "\n",
      "\n",
      " Searching with a cell #3\n",
      "*Train* [2022-11-03 06:28:35] Ep:0 [000/391] Time 0.35 (0.35) Data 0.19 (0.19) Base [Loss 2.469 (2.469)  Prec@1 25.00 (25.00) Prec@5 90.62 (90.62)]\n",
      "*Train* [2022-11-03 06:28:58] Ep:0 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.695 (0.687)  Prec@1 76.56 (76.34) Prec@5 100.00 (98.49)]\n",
      "*Train* [2022-11-03 06:29:16] Ep:0 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.919 (0.675)  Prec@1 75.00 (76.78) Prec@5 95.00 (98.59)]\n",
      "Ep:0 ends : loss=0.67, accuracy@1=76.78%, accuracy@5=98.59%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 06:29:16] Ep:1 [000/391] Time 0.28 (0.28) Data 0.20 (0.20) Base [Loss 1.238 (1.238)  Prec@1 62.50 (62.50) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:29:32] Ep:1 [200/391] Time 0.08 (0.08) Data 0.00 (0.00) Base [Loss 0.719 (0.648)  Prec@1 73.44 (77.57) Prec@5 100.00 (98.77)]\n",
      "*Train* [2022-11-03 06:29:55] Ep:1 [390/391] Time 0.12 (0.10) Data 0.00 (0.00) Base [Loss 1.082 (0.652)  Prec@1 65.00 (77.43) Prec@5 95.00 (98.68)]\n",
      "Ep:1 ends : loss=0.65, accuracy@1=77.43%, accuracy@5=98.68%\n",
      "*Train* [2022-11-03 06:29:56] Ep:2 [000/391] Time 0.30 (0.30) Data 0.16 (0.16) Base [Loss 1.180 (1.180)  Prec@1 54.69 (54.69) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:30:17] Ep:2 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.598 (0.645)  Prec@1 73.44 (77.78) Prec@5 100.00 (98.67)]\n",
      "*Train* [2022-11-03 06:30:38] Ep:2 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.824 (0.646)  Prec@1 77.50 (77.79) Prec@5 92.50 (98.70)]\n",
      "Ep:2 ends : loss=0.65, accuracy@1=77.79%, accuracy@5=98.70%\n",
      "*Train* [2022-11-03 06:30:39] Ep:3 [000/391] Time 0.23 (0.23) Data 0.15 (0.15) Base [Loss 0.874 (0.874)  Prec@1 73.44 (73.44) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 06:31:00] Ep:3 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.487 (0.660)  Prec@1 84.38 (77.38) Prec@5 100.00 (98.53)]\n",
      "*Train* [2022-11-03 06:31:22] Ep:3 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.612 (0.658)  Prec@1 77.50 (77.28) Prec@5 100.00 (98.65)]\n",
      "Ep:3 ends : loss=0.66, accuracy@1=77.28%, accuracy@5=98.65%\n",
      "*Train* [2022-11-03 06:31:23] Ep:4 [000/391] Time 0.33 (0.33) Data 0.16 (0.16) Base [Loss 0.709 (0.709)  Prec@1 71.88 (71.88) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 06:31:46] Ep:4 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.815 (0.635)  Prec@1 68.75 (77.88) Prec@5 96.88 (98.72)]\n",
      "*Train* [2022-11-03 06:32:02] Ep:4 [390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 0.597 (0.632)  Prec@1 72.50 (78.13) Prec@5 97.50 (98.73)]\n",
      "Ep:4 ends : loss=0.63, accuracy@1=78.13%, accuracy@5=98.73%\n",
      "*Train* [2022-11-03 06:32:02] Ep:5 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 3.943 (3.943)  Prec@1 7.81 (7.81) Prec@5 53.12 (53.12)]\n",
      "*Train* [2022-11-03 06:32:23] Ep:5 [200/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 0.743 (0.810)  Prec@1 73.44 (72.01) Prec@5 98.44 (97.37)]\n",
      "*Train* [2022-11-03 06:32:44] Ep:5 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.617 (0.733)  Prec@1 80.00 (74.77) Prec@5 100.00 (97.88)]\n",
      "Ep:5 ends : loss=0.73, accuracy@1=74.77%, accuracy@5=97.88%\n",
      "*Train* [2022-11-03 06:32:45] Ep:6 [000/391] Time 0.33 (0.33) Data 0.21 (0.21) Base [Loss 0.897 (0.897)  Prec@1 68.75 (68.75) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:33:08] Ep:6 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.631 (0.653)  Prec@1 81.25 (77.29) Prec@5 98.44 (98.54)]\n",
      "*Train* [2022-11-03 06:33:25] Ep:6 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.533 (0.641)  Prec@1 85.00 (77.85) Prec@5 97.50 (98.64)]\n",
      "Ep:6 ends : loss=0.64, accuracy@1=77.85%, accuracy@5=98.64%\n",
      "*Train* [2022-11-03 06:33:26] Ep:7 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 3.710 (3.710)  Prec@1 7.81 (7.81) Prec@5 56.25 (56.25)]\n",
      "*Train* [2022-11-03 06:33:50] Ep:7 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.715 (0.748)  Prec@1 71.88 (74.83) Prec@5 100.00 (97.74)]\n",
      "*Train* [2022-11-03 06:34:13] Ep:7 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.564 (0.698)  Prec@1 82.50 (76.30) Prec@5 100.00 (98.25)]\n",
      "Ep:7 ends : loss=0.70, accuracy@1=76.30%, accuracy@5=98.25%\n",
      "*Train* [2022-11-03 06:34:13] Ep:8 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 0.804 (0.804)  Prec@1 73.44 (73.44) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 06:34:37] Ep:8 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.842 (0.637)  Prec@1 73.44 (78.18) Prec@5 96.88 (98.68)]\n",
      "*Train* [2022-11-03 06:34:56] Ep:8 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.548 (0.628)  Prec@1 80.00 (78.40) Prec@5 100.00 (98.80)]\n",
      "Ep:8 ends : loss=0.63, accuracy@1=78.40%, accuracy@5=98.80%\n",
      "*Train* [2022-11-03 06:34:57] Ep:9 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 0.723 (0.723)  Prec@1 73.44 (73.44) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:35:18] Ep:9 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.633 (0.633)  Prec@1 81.25 (78.02) Prec@5 100.00 (98.80)]\n",
      "*Train* [2022-11-03 06:35:39] Ep:9 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.885 (0.629)  Prec@1 67.50 (78.12) Prec@5 100.00 (98.85)]\n",
      "Ep:9 ends : loss=0.63, accuracy@1=78.12%, accuracy@5=98.85%\n",
      "Found best op for target cell:3\n",
      ": Structure(4 nodes with |skip_connect~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|none~1|none~2|) with accuracy=75.59%, confidence=63.221%, sensitivity=0.122, robustness=-0.045, step_sim=99.764\n",
      "\n",
      "\n",
      " Searching with a cell #4\n",
      "*Train* [2022-11-03 06:38:28] Ep:0 [000/391] Time 0.28 (0.28) Data 0.19 (0.19) Base [Loss 1.986 (1.986)  Prec@1 28.12 (28.12) Prec@5 87.50 (87.50)]\n",
      "*Train* [2022-11-03 06:38:55] Ep:0 [200/391] Time 0.12 (0.13) Data 0.00 (0.00) Base [Loss 0.584 (0.655)  Prec@1 82.81 (77.54) Prec@5 100.00 (98.45)]\n",
      "*Train* [2022-11-03 06:39:17] Ep:0 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.667 (0.648)  Prec@1 77.50 (77.71) Prec@5 97.50 (98.55)]\n",
      "Ep:0 ends : loss=0.65, accuracy@1=77.71%, accuracy@5=98.55%\n",
      "*Train* [2022-11-03 06:39:17] Ep:1 [000/391] Time 0.31 (0.31) Data 0.16 (0.16) Base [Loss 3.702 (3.702)  Prec@1 10.94 (10.94) Prec@5 48.44 (48.44)]\n",
      "*Train* [2022-11-03 06:39:40] Ep:1 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.632 (0.735)  Prec@1 81.25 (75.34) Prec@5 98.44 (97.61)]\n",
      "*Train* [2022-11-03 06:40:01] Ep:1 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.573 (0.679)  Prec@1 77.50 (77.06) Prec@5 100.00 (98.16)]\n",
      "Ep:1 ends : loss=0.68, accuracy@1=77.06%, accuracy@5=98.16%\n",
      "*Train* [2022-11-03 06:40:01] Ep:2 [000/391] Time 0.31 (0.31) Data 0.17 (0.17) Base [Loss 1.778 (1.778)  Prec@1 43.75 (43.75) Prec@5 90.62 (90.62)]\n",
      "*Train* [2022-11-03 06:40:27] Ep:2 [200/391] Time 0.12 (0.13) Data 0.00 (0.00) Base [Loss 0.563 (0.644)  Prec@1 81.25 (77.77) Prec@5 98.44 (98.81)]\n",
      "*Train* [2022-11-03 06:40:49] Ep:2 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.352 (0.636)  Prec@1 87.50 (78.02) Prec@5 100.00 (98.83)]\n",
      "Ep:2 ends : loss=0.64, accuracy@1=78.02%, accuracy@5=98.83%\n",
      "*Train* [2022-11-03 06:40:49] Ep:3 [000/391] Time 0.34 (0.34) Data 0.19 (0.19) Base [Loss 3.572 (3.572)  Prec@1 10.94 (10.94) Prec@5 57.81 (57.81)]\n",
      "*Train* [2022-11-03 06:41:10] Ep:3 [200/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.668 (0.699)  Prec@1 78.12 (75.87) Prec@5 98.44 (98.27)]\n",
      "*Train* [2022-11-03 06:41:31] Ep:3 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.559 (0.669)  Prec@1 77.50 (76.74) Prec@5 100.00 (98.51)]\n",
      "Ep:3 ends : loss=0.67, accuracy@1=76.74%, accuracy@5=98.51%\n",
      "*Train* [2022-11-03 06:41:31] Ep:4 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 0.883 (0.883)  Prec@1 64.06 (64.06) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:41:50] Ep:4 [200/391] Time 0.08 (0.09) Data 0.00 (0.00) Base [Loss 0.628 (0.622)  Prec@1 78.12 (78.78) Prec@5 100.00 (98.73)]\n",
      "*Train* [2022-11-03 06:42:11] Ep:4 [390/391] Time 0.13 (0.10) Data 0.00 (0.00) Base [Loss 0.607 (0.619)  Prec@1 80.00 (78.56) Prec@5 100.00 (98.81)]\n",
      "Ep:4 ends : loss=0.62, accuracy@1=78.56%, accuracy@5=98.81%\n",
      "*Train* [2022-11-03 06:42:12] Ep:5 [000/391] Time 0.37 (0.37) Data 0.23 (0.23) Base [Loss 0.696 (0.696)  Prec@1 78.12 (78.12) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 06:42:34] Ep:5 [200/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.740 (0.619)  Prec@1 73.44 (78.94) Prec@5 98.44 (98.76)]\n",
      "*Train* [2022-11-03 06:42:55] Ep:5 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.822 (0.618)  Prec@1 70.00 (78.96) Prec@5 97.50 (98.79)]\n",
      "Ep:5 ends : loss=0.62, accuracy@1=78.96%, accuracy@5=98.79%\n",
      "*Train* [2022-11-03 06:42:55] Ep:6 [000/391] Time 0.30 (0.30) Data 0.21 (0.21) Base [Loss 0.743 (0.743)  Prec@1 78.12 (78.12) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:43:18] Ep:6 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.671 (0.626)  Prec@1 75.00 (78.42) Prec@5 100.00 (98.83)]\n",
      "*Train* [2022-11-03 06:43:38] Ep:6 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.608 (0.605)  Prec@1 80.00 (79.06) Prec@5 100.00 (98.89)]\n",
      "Ep:6 ends : loss=0.61, accuracy@1=79.06%, accuracy@5=98.89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 06:43:38] Ep:7 [000/391] Time 0.28 (0.28) Data 0.17 (0.17) Base [Loss 0.665 (0.665)  Prec@1 75.00 (75.00) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:44:02] Ep:7 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.612 (0.601)  Prec@1 76.56 (79.50) Prec@5 98.44 (98.78)]\n",
      "*Train* [2022-11-03 06:44:25] Ep:7 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.707 (0.600)  Prec@1 72.50 (79.47) Prec@5 100.00 (98.82)]\n",
      "Ep:7 ends : loss=0.60, accuracy@1=79.47%, accuracy@5=98.82%\n",
      "*Train* [2022-11-03 06:44:26] Ep:8 [000/391] Time 0.28 (0.28) Data 0.18 (0.18) Base [Loss 0.826 (0.826)  Prec@1 67.19 (67.19) Prec@5 93.75 (93.75)]\n",
      "*Train* [2022-11-03 06:44:49] Ep:8 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.635 (0.601)  Prec@1 78.12 (78.89) Prec@5 100.00 (99.11)]\n",
      "*Train* [2022-11-03 06:45:08] Ep:8 [390/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.880 (0.601)  Prec@1 70.00 (79.21) Prec@5 100.00 (98.96)]\n",
      "Ep:8 ends : loss=0.60, accuracy@1=79.21%, accuracy@5=98.96%\n",
      "*Train* [2022-11-03 06:45:09] Ep:9 [000/391] Time 0.26 (0.26) Data 0.18 (0.18) Base [Loss 1.745 (1.745)  Prec@1 42.19 (42.19) Prec@5 89.06 (89.06)]\n",
      "*Train* [2022-11-03 06:45:30] Ep:9 [200/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.416 (0.632)  Prec@1 92.19 (78.26) Prec@5 98.44 (98.64)]\n",
      "*Train* [2022-11-03 06:45:49] Ep:9 [390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 0.706 (0.617)  Prec@1 72.50 (78.95) Prec@5 97.50 (98.74)]\n",
      "Ep:9 ends : loss=0.62, accuracy@1=78.95%, accuracy@5=98.74%\n",
      "Found best op for target cell:4\n",
      ": Structure(4 nodes with |skip_connect~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|nor_conv_3x3~0|avg_pool_3x3~1|nor_conv_1x1~2|) with accuracy=73.63%, confidence=63.497%, sensitivity=0.105, robustness=-0.050, step_sim=99.627\n",
      "\n",
      "\n",
      " Searching with a cell #5\n",
      "*Train* [2022-11-03 06:48:34] Ep:0 [000/391] Time 0.24 (0.24) Data 0.16 (0.16) Base [Loss 3.843 (3.843)  Prec@1 7.81 (7.81) Prec@5 39.06 (39.06)]\n",
      "*Train* [2022-11-03 06:48:57] Ep:0 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.693 (0.830)  Prec@1 82.81 (71.65) Prec@5 98.44 (97.43)]\n",
      "*Train* [2022-11-03 06:49:20] Ep:0 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.808 (0.744)  Prec@1 67.50 (74.57) Prec@5 100.00 (97.94)]\n",
      "Ep:0 ends : loss=0.74, accuracy@1=74.57%, accuracy@5=97.94%\n",
      "*Train* [2022-11-03 06:49:21] Ep:1 [000/391] Time 0.29 (0.29) Data 0.19 (0.19) Base [Loss 1.801 (1.801)  Prec@1 32.81 (32.81) Prec@5 90.62 (90.62)]\n",
      "*Train* [2022-11-03 06:49:42] Ep:1 [200/391] Time 0.11 (0.11) Data 0.00 (0.00) Base [Loss 0.701 (0.713)  Prec@1 76.56 (75.57) Prec@5 95.31 (98.38)]\n",
      "*Train* [2022-11-03 06:50:04] Ep:1 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.539 (0.667)  Prec@1 80.00 (76.94) Prec@5 97.50 (98.62)]\n",
      "Ep:1 ends : loss=0.67, accuracy@1=76.94%, accuracy@5=98.62%\n",
      "*Train* [2022-11-03 06:50:04] Ep:2 [000/391] Time 0.25 (0.25) Data 0.16 (0.16) Base [Loss 1.155 (1.155)  Prec@1 57.81 (57.81) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:50:25] Ep:2 [200/391] Time 0.12 (0.10) Data 0.00 (0.00) Base [Loss 0.648 (0.634)  Prec@1 75.00 (78.20) Prec@5 100.00 (98.83)]\n",
      "*Train* [2022-11-03 06:50:47] Ep:2 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.651 (0.622)  Prec@1 77.50 (78.53) Prec@5 97.50 (98.82)]\n",
      "Ep:2 ends : loss=0.62, accuracy@1=78.53%, accuracy@5=98.82%\n",
      "*Train* [2022-11-03 06:50:47] Ep:3 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 0.854 (0.854)  Prec@1 76.56 (76.56) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:51:10] Ep:3 [200/391] Time 0.11 (0.12) Data 0.00 (0.00) Base [Loss 0.717 (0.662)  Prec@1 75.00 (76.85) Prec@5 98.44 (98.55)]\n",
      "*Train* [2022-11-03 06:51:31] Ep:3 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.747 (0.647)  Prec@1 82.50 (77.65) Prec@5 97.50 (98.64)]\n",
      "Ep:3 ends : loss=0.65, accuracy@1=77.65%, accuracy@5=98.64%\n",
      "*Train* [2022-11-03 06:51:32] Ep:4 [000/391] Time 0.29 (0.29) Data 0.20 (0.20) Base [Loss 0.686 (0.686)  Prec@1 76.56 (76.56) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:51:56] Ep:4 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.566 (0.614)  Prec@1 78.12 (79.36) Prec@5 100.00 (98.83)]\n",
      "*Train* [2022-11-03 06:52:15] Ep:4 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.592 (0.602)  Prec@1 77.50 (79.66) Prec@5 97.50 (98.83)]\n",
      "Ep:4 ends : loss=0.60, accuracy@1=79.66%, accuracy@5=98.83%\n",
      "*Train* [2022-11-03 06:52:16] Ep:5 [000/391] Time 0.25 (0.25) Data 0.15 (0.15) Base [Loss 0.659 (0.659)  Prec@1 73.44 (73.44) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 06:52:38] Ep:5 [200/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.476 (0.594)  Prec@1 84.38 (79.49) Prec@5 98.44 (98.94)]\n",
      "*Train* [2022-11-03 06:52:59] Ep:5 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.810 (0.584)  Prec@1 75.00 (79.92) Prec@5 97.50 (98.96)]\n",
      "Ep:5 ends : loss=0.58, accuracy@1=79.92%, accuracy@5=98.96%\n",
      "*Train* [2022-11-03 06:52:59] Ep:6 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 4.336 (4.336)  Prec@1 6.25 (6.25) Prec@5 31.25 (31.25)]\n",
      "*Train* [2022-11-03 06:53:23] Ep:6 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.623 (0.920)  Prec@1 71.88 (68.09) Prec@5 100.00 (96.84)]\n",
      "*Train* [2022-11-03 06:53:46] Ep:6 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.630 (0.792)  Prec@1 72.50 (72.32) Prec@5 95.00 (97.75)]\n",
      "Ep:6 ends : loss=0.79, accuracy@1=72.32%, accuracy@5=97.75%\n",
      "*Train* [2022-11-03 06:53:47] Ep:7 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 0.674 (0.674)  Prec@1 81.25 (81.25) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 06:54:09] Ep:7 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.647 (0.627)  Prec@1 84.38 (78.56) Prec@5 96.88 (98.85)]\n",
      "*Train* [2022-11-03 06:54:31] Ep:7 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.800 (0.609)  Prec@1 72.50 (79.02) Prec@5 100.00 (98.91)]\n",
      "Ep:7 ends : loss=0.61, accuracy@1=79.02%, accuracy@5=98.91%\n",
      "*Train* [2022-11-03 06:54:32] Ep:8 [000/391] Time 0.35 (0.35) Data 0.20 (0.20) Base [Loss 1.530 (1.530)  Prec@1 42.19 (42.19) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 06:54:52] Ep:8 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.719 (0.655)  Prec@1 79.69 (77.50) Prec@5 96.88 (98.69)]\n",
      "*Train* [2022-11-03 06:55:13] Ep:8 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.629 (0.634)  Prec@1 75.00 (78.30) Prec@5 97.50 (98.70)]\n",
      "Ep:8 ends : loss=0.63, accuracy@1=78.30%, accuracy@5=98.70%\n",
      "*Train* [2022-11-03 06:55:13] Ep:9 [000/391] Time 0.29 (0.29) Data 0.17 (0.17) Base [Loss 0.735 (0.735)  Prec@1 75.00 (75.00) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 06:55:35] Ep:9 [200/391] Time 0.11 (0.11) Data 0.00 (0.00) Base [Loss 0.593 (0.593)  Prec@1 81.25 (79.77) Prec@5 98.44 (98.98)]\n",
      "*Train* [2022-11-03 06:55:58] Ep:9 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.641 (0.595)  Prec@1 77.50 (79.68) Prec@5 100.00 (98.94)]\n",
      "Ep:9 ends : loss=0.60, accuracy@1=79.68%, accuracy@5=98.94%\n",
      "Found best op for target cell:5\n",
      ": Structure(4 nodes with |skip_connect~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_3x3~2|) with accuracy=74.80%, confidence=63.465%, sensitivity=0.149, robustness=-0.050, step_sim=99.741\n",
      "\n",
      "\n",
      " Searching with a cell #6\n",
      "*Train* [2022-11-03 06:58:54] Ep:0 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 3.059 (3.059)  Prec@1 14.06 (14.06) Prec@5 65.62 (65.62)]\n",
      "*Train* [2022-11-03 06:59:17] Ep:0 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.429 (0.774)  Prec@1 90.62 (73.34) Prec@5 100.00 (97.75)]\n",
      "*Train* [2022-11-03 06:59:39] Ep:0 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.772 (0.696)  Prec@1 67.50 (75.99) Prec@5 100.00 (98.27)]\n",
      "Ep:0 ends : loss=0.70, accuracy@1=75.99%, accuracy@5=98.27%\n",
      "*Train* [2022-11-03 06:59:40] Ep:1 [000/391] Time 0.30 (0.30) Data 0.19 (0.19) Base [Loss 3.871 (3.871)  Prec@1 12.50 (12.50) Prec@5 46.88 (46.88)]\n",
      "*Train* [2022-11-03 07:00:01] Ep:1 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.621 (0.744)  Prec@1 76.56 (75.03) Prec@5 98.44 (97.99)]\n",
      "*Train* [2022-11-03 07:00:24] Ep:1 [390/391] Time 0.11 (0.11) Data 0.00 (0.00) Base [Loss 0.445 (0.675)  Prec@1 80.00 (77.16) Prec@5 100.00 (98.42)]\n",
      "Ep:1 ends : loss=0.68, accuracy@1=77.16%, accuracy@5=98.42%\n",
      "*Train* [2022-11-03 07:00:25] Ep:2 [000/391] Time 0.33 (0.33) Data 0.18 (0.18) Base [Loss 1.355 (1.355)  Prec@1 50.00 (50.00) Prec@5 93.75 (93.75)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 07:00:46] Ep:2 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.656 (0.624)  Prec@1 81.25 (78.53) Prec@5 96.88 (98.76)]\n",
      "*Train* [2022-11-03 07:01:07] Ep:2 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.698 (0.611)  Prec@1 75.00 (78.98) Prec@5 100.00 (98.76)]\n",
      "Ep:2 ends : loss=0.61, accuracy@1=78.98%, accuracy@5=98.76%\n",
      "*Train* [2022-11-03 07:01:07] Ep:3 [000/391] Time 0.25 (0.25) Data 0.16 (0.16) Base [Loss 0.916 (0.916)  Prec@1 70.31 (70.31) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:01:31] Ep:3 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.522 (0.627)  Prec@1 79.69 (78.35) Prec@5 100.00 (98.79)]\n",
      "*Train* [2022-11-03 07:01:55] Ep:3 [390/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.563 (0.615)  Prec@1 80.00 (78.92) Prec@5 100.00 (98.81)]\n",
      "Ep:3 ends : loss=0.62, accuracy@1=78.92%, accuracy@5=98.81%\n",
      "*Train* [2022-11-03 07:01:55] Ep:4 [000/391] Time 0.27 (0.27) Data 0.17 (0.17) Base [Loss 1.050 (1.050)  Prec@1 59.38 (59.38) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:02:20] Ep:4 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.629 (0.636)  Prec@1 81.25 (78.26) Prec@5 100.00 (98.76)]\n",
      "*Train* [2022-11-03 07:02:42] Ep:4 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.600 (0.614)  Prec@1 82.50 (79.12) Prec@5 100.00 (98.81)]\n",
      "Ep:4 ends : loss=0.61, accuracy@1=79.12%, accuracy@5=98.81%\n",
      "*Train* [2022-11-03 07:02:42] Ep:5 [000/391] Time 0.25 (0.25) Data 0.16 (0.16) Base [Loss 0.841 (0.841)  Prec@1 65.62 (65.62) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 07:03:03] Ep:5 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.374 (0.583)  Prec@1 92.19 (80.12) Prec@5 100.00 (99.03)]\n",
      "*Train* [2022-11-03 07:03:25] Ep:5 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.688 (0.583)  Prec@1 75.00 (80.22) Prec@5 100.00 (98.97)]\n",
      "Ep:5 ends : loss=0.58, accuracy@1=80.22%, accuracy@5=98.97%\n",
      "*Train* [2022-11-03 07:03:26] Ep:6 [000/391] Time 0.29 (0.29) Data 0.20 (0.20) Base [Loss 0.662 (0.662)  Prec@1 81.25 (81.25) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:03:49] Ep:6 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.418 (0.576)  Prec@1 87.50 (80.01) Prec@5 100.00 (99.09)]\n",
      "*Train* [2022-11-03 07:04:12] Ep:6 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.627 (0.576)  Prec@1 75.00 (80.08) Prec@5 97.50 (98.99)]\n",
      "Ep:6 ends : loss=0.58, accuracy@1=80.08%, accuracy@5=98.99%\n",
      "*Train* [2022-11-03 07:04:12] Ep:7 [000/391] Time 0.33 (0.33) Data 0.17 (0.17) Base [Loss 0.941 (0.941)  Prec@1 70.31 (70.31) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:04:33] Ep:7 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.431 (0.586)  Prec@1 84.38 (79.72) Prec@5 98.44 (98.94)]\n",
      "*Train* [2022-11-03 07:04:55] Ep:7 [390/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.638 (0.572)  Prec@1 75.00 (80.22) Prec@5 97.50 (98.91)]\n",
      "Ep:7 ends : loss=0.57, accuracy@1=80.22%, accuracy@5=98.91%\n",
      "*Train* [2022-11-03 07:04:56] Ep:8 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 0.676 (0.676)  Prec@1 76.56 (76.56) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:05:18] Ep:8 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.499 (0.552)  Prec@1 82.81 (80.94) Prec@5 100.00 (99.04)]\n",
      "*Train* [2022-11-03 07:05:42] Ep:8 [390/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.404 (0.550)  Prec@1 87.50 (81.04) Prec@5 97.50 (99.07)]\n",
      "Ep:8 ends : loss=0.55, accuracy@1=81.04%, accuracy@5=99.07%\n",
      "*Train* [2022-11-03 07:05:42] Ep:9 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 0.805 (0.805)  Prec@1 76.56 (76.56) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 07:06:08] Ep:9 [200/391] Time 0.08 (0.13) Data 0.00 (0.00) Base [Loss 0.602 (0.602)  Prec@1 82.81 (79.13) Prec@5 95.31 (98.84)]\n",
      "*Train* [2022-11-03 07:06:27] Ep:9 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.492 (0.581)  Prec@1 87.50 (79.89) Prec@5 100.00 (98.90)]\n",
      "Ep:9 ends : loss=0.58, accuracy@1=79.89%, accuracy@5=98.90%\n",
      "Found best op for target cell:6\n",
      ": Structure(4 nodes with |skip_connect~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|nor_conv_1x1~0|avg_pool_3x3~1|none~2|) with accuracy=73.05%, confidence=62.344%, sensitivity=0.129, robustness=-0.060, step_sim=99.810\n",
      "\n",
      "\n",
      " Searching with a cell #7\n",
      "*Train* [2022-11-03 07:09:21] Ep:0 [000/391] Time 0.27 (0.27) Data 0.17 (0.17) Base [Loss 0.622 (0.622)  Prec@1 81.25 (81.25) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 07:09:42] Ep:0 [200/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.450 (0.594)  Prec@1 84.38 (79.53) Prec@5 100.00 (98.85)]\n",
      "*Train* [2022-11-03 07:10:03] Ep:0 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.653 (0.582)  Prec@1 75.00 (80.01) Prec@5 97.50 (98.92)]\n",
      "Ep:0 ends : loss=0.58, accuracy@1=80.01%, accuracy@5=98.92%\n",
      "*Train* [2022-11-03 07:10:03] Ep:1 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 3.638 (3.638)  Prec@1 10.94 (10.94) Prec@5 57.81 (57.81)]\n",
      "*Train* [2022-11-03 07:10:25] Ep:1 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.712 (0.741)  Prec@1 76.56 (74.64) Prec@5 100.00 (98.15)]\n",
      "*Train* [2022-11-03 07:10:48] Ep:1 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.410 (0.672)  Prec@1 87.50 (76.94) Prec@5 100.00 (98.50)]\n",
      "Ep:1 ends : loss=0.67, accuracy@1=76.94%, accuracy@5=98.50%\n",
      "*Train* [2022-11-03 07:10:48] Ep:2 [000/391] Time 0.30 (0.30) Data 0.19 (0.19) Base [Loss 1.071 (1.071)  Prec@1 67.19 (67.19) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:11:11] Ep:2 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.408 (0.583)  Prec@1 85.94 (80.11) Prec@5 100.00 (98.99)]\n",
      "*Train* [2022-11-03 07:11:29] Ep:2 [390/391] Time 0.09 (0.10) Data 0.00 (0.00) Base [Loss 0.557 (0.575)  Prec@1 80.00 (80.37) Prec@5 97.50 (98.97)]\n",
      "Ep:2 ends : loss=0.58, accuracy@1=80.37%, accuracy@5=98.97%\n",
      "*Train* [2022-11-03 07:11:29] Ep:3 [000/391] Time 0.27 (0.27) Data 0.15 (0.15) Base [Loss 0.754 (0.754)  Prec@1 73.44 (73.44) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:11:51] Ep:3 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.368 (0.561)  Prec@1 87.50 (80.84) Prec@5 100.00 (98.94)]\n",
      "*Train* [2022-11-03 07:12:14] Ep:3 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.753 (0.558)  Prec@1 72.50 (80.94) Prec@5 100.00 (98.92)]\n",
      "Ep:3 ends : loss=0.56, accuracy@1=80.94%, accuracy@5=98.92%\n",
      "*Train* [2022-11-03 07:12:14] Ep:4 [000/391] Time 0.26 (0.26) Data 0.16 (0.16) Base [Loss 4.268 (4.268)  Prec@1 4.69 (4.69) Prec@5 39.06 (39.06)]\n",
      "*Train* [2022-11-03 07:12:36] Ep:4 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.516 (0.820)  Prec@1 78.12 (71.94) Prec@5 100.00 (97.45)]\n",
      "*Train* [2022-11-03 07:12:59] Ep:4 [390/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.772 (0.709)  Prec@1 72.50 (75.84) Prec@5 100.00 (98.14)]\n",
      "Ep:4 ends : loss=0.71, accuracy@1=75.84%, accuracy@5=98.14%\n",
      "*Train* [2022-11-03 07:12:59] Ep:5 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 0.750 (0.750)  Prec@1 79.69 (79.69) Prec@5 93.75 (93.75)]\n",
      "*Train* [2022-11-03 07:13:22] Ep:5 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.501 (0.566)  Prec@1 84.38 (80.57) Prec@5 100.00 (98.94)]\n",
      "*Train* [2022-11-03 07:13:45] Ep:5 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.307 (0.555)  Prec@1 90.00 (80.88) Prec@5 100.00 (99.02)]\n",
      "Ep:5 ends : loss=0.55, accuracy@1=80.88%, accuracy@5=99.02%\n",
      "*Train* [2022-11-03 07:13:46] Ep:6 [000/391] Time 0.34 (0.34) Data 0.19 (0.19) Base [Loss 0.992 (0.992)  Prec@1 67.19 (67.19) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 07:14:09] Ep:6 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.730 (0.612)  Prec@1 78.12 (78.61) Prec@5 98.44 (98.87)]\n",
      "*Train* [2022-11-03 07:14:33] Ep:6 [390/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.607 (0.598)  Prec@1 77.50 (79.41) Prec@5 97.50 (98.89)]\n",
      "Ep:6 ends : loss=0.60, accuracy@1=79.41%, accuracy@5=98.89%\n",
      "*Train* [2022-11-03 07:14:33] Ep:7 [000/391] Time 0.31 (0.31) Data 0.20 (0.20) Base [Loss 0.697 (0.697)  Prec@1 81.25 (81.25) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:14:56] Ep:7 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.440 (0.571)  Prec@1 82.81 (80.33) Prec@5 98.44 (99.10)]\n",
      "*Train* [2022-11-03 07:15:20] Ep:7 [390/391] Time 0.11 (0.12) Data 0.00 (0.00) Base [Loss 0.598 (0.563)  Prec@1 80.00 (80.64) Prec@5 97.50 (99.08)]\n",
      "Ep:7 ends : loss=0.56, accuracy@1=80.64%, accuracy@5=99.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 07:15:20] Ep:8 [000/391] Time 0.28 (0.28) Data 0.19 (0.19) Base [Loss 0.577 (0.577)  Prec@1 79.69 (79.69) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:15:41] Ep:8 [200/391] Time 0.08 (0.10) Data 0.00 (0.00) Base [Loss 0.388 (0.560)  Prec@1 85.94 (80.69) Prec@5 100.00 (99.11)]\n",
      "*Train* [2022-11-03 07:16:03] Ep:8 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.866 (0.548)  Prec@1 70.00 (81.14) Prec@5 97.50 (99.10)]\n",
      "Ep:8 ends : loss=0.55, accuracy@1=81.14%, accuracy@5=99.10%\n",
      "*Train* [2022-11-03 07:16:03] Ep:9 [000/391] Time 0.28 (0.28) Data 0.18 (0.18) Base [Loss 0.698 (0.698)  Prec@1 78.12 (78.12) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:16:27] Ep:9 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.568 (0.550)  Prec@1 84.38 (80.87) Prec@5 98.44 (99.03)]\n",
      "*Train* [2022-11-03 07:16:47] Ep:9 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.524 (0.538)  Prec@1 80.00 (81.44) Prec@5 97.50 (99.07)]\n",
      "Ep:9 ends : loss=0.54, accuracy@1=81.44%, accuracy@5=99.07%\n",
      "Found best op for target cell:7\n",
      ": Structure(4 nodes with |skip_connect~0|+|avg_pool_3x3~0|none~1|+|nor_conv_3x3~0|avg_pool_3x3~1|none~2|) with accuracy=71.88%, confidence=63.529%, sensitivity=0.182, robustness=-0.095, step_sim=99.675\n",
      "\n",
      "\n",
      " Searching with a cell #8\n",
      "*Train* [2022-11-03 07:19:50] Ep:0 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 0.853 (0.853)  Prec@1 71.88 (71.88) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:20:12] Ep:0 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.368 (0.588)  Prec@1 87.50 (79.72) Prec@5 100.00 (98.95)]\n",
      "*Train* [2022-11-03 07:20:35] Ep:0 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.346 (0.578)  Prec@1 85.00 (80.08) Prec@5 100.00 (98.91)]\n",
      "Ep:0 ends : loss=0.58, accuracy@1=80.08%, accuracy@5=98.91%\n",
      "*Train* [2022-11-03 07:20:35] Ep:1 [000/391] Time 0.37 (0.37) Data 0.20 (0.20) Base [Loss 3.140 (3.140)  Prec@1 18.75 (18.75) Prec@5 57.81 (57.81)]\n",
      "*Train* [2022-11-03 07:20:56] Ep:1 [200/391] Time 0.14 (0.10) Data 0.00 (0.00) Base [Loss 0.656 (0.677)  Prec@1 78.12 (76.77) Prec@5 98.44 (98.25)]\n",
      "*Train* [2022-11-03 07:21:18] Ep:1 [390/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.939 (0.626)  Prec@1 60.00 (78.43) Prec@5 97.50 (98.64)]\n",
      "Ep:1 ends : loss=0.63, accuracy@1=78.43%, accuracy@5=98.64%\n",
      "*Train* [2022-11-03 07:21:18] Ep:2 [000/391] Time 0.29 (0.29) Data 0.19 (0.19) Base [Loss 0.687 (0.687)  Prec@1 75.00 (75.00) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:21:43] Ep:2 [200/391] Time 0.07 (0.13) Data 0.00 (0.00) Base [Loss 0.388 (0.527)  Prec@1 85.94 (81.77) Prec@5 100.00 (99.21)]\n",
      "*Train* [2022-11-03 07:22:05] Ep:2 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.324 (0.526)  Prec@1 87.50 (81.86) Prec@5 100.00 (99.12)]\n",
      "Ep:2 ends : loss=0.53, accuracy@1=81.86%, accuracy@5=99.12%\n",
      "*Train* [2022-11-03 07:22:05] Ep:3 [000/391] Time 0.28 (0.28) Data 0.17 (0.17) Base [Loss 0.562 (0.562)  Prec@1 84.38 (84.38) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:22:30] Ep:3 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.453 (0.549)  Prec@1 84.38 (80.95) Prec@5 98.44 (99.03)]\n",
      "*Train* [2022-11-03 07:22:50] Ep:3 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.478 (0.543)  Prec@1 82.50 (81.19) Prec@5 100.00 (99.14)]\n",
      "Ep:3 ends : loss=0.54, accuracy@1=81.19%, accuracy@5=99.14%\n",
      "*Train* [2022-11-03 07:22:50] Ep:4 [000/391] Time 0.28 (0.28) Data 0.20 (0.20) Base [Loss 0.571 (0.571)  Prec@1 82.81 (82.81) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:23:13] Ep:4 [200/391] Time 0.11 (0.12) Data 0.00 (0.00) Base [Loss 0.346 (0.531)  Prec@1 87.50 (82.02) Prec@5 100.00 (99.21)]\n",
      "*Train* [2022-11-03 07:23:34] Ep:4 [390/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.673 (0.526)  Prec@1 75.00 (81.95) Prec@5 100.00 (99.19)]\n",
      "Ep:4 ends : loss=0.53, accuracy@1=81.95%, accuracy@5=99.19%\n",
      "*Train* [2022-11-03 07:23:34] Ep:5 [000/391] Time 0.28 (0.28) Data 0.18 (0.18) Base [Loss 3.665 (3.665)  Prec@1 14.06 (14.06) Prec@5 53.12 (53.12)]\n",
      "*Train* [2022-11-03 07:23:57] Ep:5 [200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.714 (0.793)  Prec@1 81.25 (72.57) Prec@5 96.88 (97.68)]\n",
      "*Train* [2022-11-03 07:24:18] Ep:5 [390/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.608 (0.703)  Prec@1 75.00 (75.76) Prec@5 100.00 (98.25)]\n",
      "Ep:5 ends : loss=0.70, accuracy@1=75.76%, accuracy@5=98.25%\n",
      "*Train* [2022-11-03 07:24:18] Ep:6 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 0.730 (0.730)  Prec@1 79.69 (79.69) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:24:42] Ep:6 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.487 (0.544)  Prec@1 82.81 (81.51) Prec@5 100.00 (99.17)]\n",
      "*Train* [2022-11-03 07:25:06] Ep:6 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.538 (0.544)  Prec@1 80.00 (81.34) Prec@5 100.00 (99.15)]\n",
      "Ep:6 ends : loss=0.54, accuracy@1=81.34%, accuracy@5=99.15%\n",
      "*Train* [2022-11-03 07:25:06] Ep:7 [000/391] Time 0.35 (0.35) Data 0.19 (0.19) Base [Loss 0.782 (0.782)  Prec@1 73.44 (73.44) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 07:25:28] Ep:7 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.319 (0.574)  Prec@1 90.62 (80.29) Prec@5 100.00 (99.04)]\n",
      "*Train* [2022-11-03 07:25:51] Ep:7 [390/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.555 (0.568)  Prec@1 82.50 (80.42) Prec@5 100.00 (98.98)]\n",
      "Ep:7 ends : loss=0.57, accuracy@1=80.42%, accuracy@5=98.98%\n",
      "*Train* [2022-11-03 07:25:51] Ep:8 [000/391] Time 0.29 (0.29) Data 0.19 (0.19) Base [Loss 0.503 (0.503)  Prec@1 79.69 (79.69) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:26:15] Ep:8 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.571 (0.523)  Prec@1 81.25 (82.07) Prec@5 98.44 (99.07)]\n",
      "*Train* [2022-11-03 07:26:37] Ep:8 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.765 (0.522)  Prec@1 67.50 (81.96) Prec@5 100.00 (99.13)]\n",
      "Ep:8 ends : loss=0.52, accuracy@1=81.96%, accuracy@5=99.13%\n",
      "*Train* [2022-11-03 07:26:37] Ep:9 [000/391] Time 0.25 (0.25) Data 0.16 (0.16) Base [Loss 0.464 (0.464)  Prec@1 82.81 (82.81) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:26:59] Ep:9 [200/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.506 (0.539)  Prec@1 84.38 (81.26) Prec@5 98.44 (99.18)]\n",
      "*Train* [2022-11-03 07:27:23] Ep:9 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.484 (0.538)  Prec@1 87.50 (81.55) Prec@5 97.50 (99.16)]\n",
      "Ep:9 ends : loss=0.54, accuracy@1=81.55%, accuracy@5=99.16%\n",
      "Found best op for target cell:8\n",
      ": Structure(4 nodes with |skip_connect~0|+|nor_conv_1x1~0|skip_connect~1|+|nor_conv_3x3~0|avg_pool_3x3~1|none~2|) with accuracy=73.24%, confidence=64.528%, sensitivity=0.118, robustness=-0.070, step_sim=99.765\n",
      "\n",
      "\n",
      " Searching with a cell #9\n",
      "*Train* [2022-11-03 07:30:33] Ep:0 [000/391] Time 0.34 (0.34) Data 0.18 (0.18) Base [Loss 3.981 (3.981)  Prec@1 12.50 (12.50) Prec@5 45.31 (45.31)]\n",
      "*Train* [2022-11-03 07:30:55] Ep:0 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.825 (0.733)  Prec@1 71.88 (74.74) Prec@5 98.44 (98.06)]\n",
      "*Train* [2022-11-03 07:31:22] Ep:0 [390/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.455 (0.656)  Prec@1 82.50 (77.49) Prec@5 100.00 (98.49)]\n",
      "Ep:0 ends : loss=0.66, accuracy@1=77.49%, accuracy@5=98.49%\n",
      "*Train* [2022-11-03 07:31:22] Ep:1 [000/391] Time 0.25 (0.25) Data 0.16 (0.16) Base [Loss 0.665 (0.665)  Prec@1 71.88 (71.88) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:31:46] Ep:1 [200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.513 (0.527)  Prec@1 81.25 (81.95) Prec@5 98.44 (99.07)]\n",
      "*Train* [2022-11-03 07:32:11] Ep:1 [390/391] Time 0.09 (0.13) Data 0.00 (0.00) Base [Loss 0.566 (0.531)  Prec@1 80.00 (81.82) Prec@5 100.00 (99.06)]\n",
      "Ep:1 ends : loss=0.53, accuracy@1=81.82%, accuracy@5=99.06%\n",
      "*Train* [2022-11-03 07:32:11] Ep:2 [000/391] Time 0.35 (0.35) Data 0.20 (0.20) Base [Loss 0.558 (0.558)  Prec@1 87.50 (87.50) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:32:34] Ep:2 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.442 (0.536)  Prec@1 87.50 (81.71) Prec@5 98.44 (99.11)]\n",
      "*Train* [2022-11-03 07:32:57] Ep:2 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.448 (0.529)  Prec@1 85.00 (81.98) Prec@5 97.50 (99.17)]\n",
      "Ep:2 ends : loss=0.53, accuracy@1=81.98%, accuracy@5=99.17%\n",
      "*Train* [2022-11-03 07:32:58] Ep:3 [000/391] Time 0.24 (0.24) Data 0.15 (0.15) Base [Loss 0.574 (0.574)  Prec@1 84.38 (84.38) Prec@5 100.00 (100.00)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 07:33:19] Ep:3 [200/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.378 (0.534)  Prec@1 87.50 (82.13) Prec@5 100.00 (99.09)]\n",
      "*Train* [2022-11-03 07:33:42] Ep:3 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.468 (0.543)  Prec@1 85.00 (81.59) Prec@5 100.00 (99.10)]\n",
      "Ep:3 ends : loss=0.54, accuracy@1=81.59%, accuracy@5=99.10%\n",
      "*Train* [2022-11-03 07:33:42] Ep:4 [000/391] Time 0.35 (0.35) Data 0.19 (0.19) Base [Loss 2.132 (2.132)  Prec@1 37.50 (37.50) Prec@5 82.81 (82.81)]\n",
      "*Train* [2022-11-03 07:34:06] Ep:4 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.743 (0.629)  Prec@1 81.25 (78.12) Prec@5 96.88 (98.72)]\n",
      "*Train* [2022-11-03 07:34:28] Ep:4 [390/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.566 (0.593)  Prec@1 82.50 (79.50) Prec@5 100.00 (98.92)]\n",
      "Ep:4 ends : loss=0.59, accuracy@1=79.50%, accuracy@5=98.92%\n",
      "*Train* [2022-11-03 07:34:28] Ep:5 [000/391] Time 0.28 (0.28) Data 0.19 (0.19) Base [Loss 0.616 (0.616)  Prec@1 79.69 (79.69) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:34:50] Ep:5 [200/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.421 (0.520)  Prec@1 82.81 (82.17) Prec@5 100.00 (99.02)]\n",
      "*Train* [2022-11-03 07:35:12] Ep:5 [390/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.290 (0.520)  Prec@1 85.00 (82.00) Prec@5 100.00 (99.08)]\n",
      "Ep:5 ends : loss=0.52, accuracy@1=82.00%, accuracy@5=99.08%\n",
      "*Train* [2022-11-03 07:35:12] Ep:6 [000/391] Time 0.37 (0.37) Data 0.21 (0.21) Base [Loss 0.788 (0.788)  Prec@1 81.25 (81.25) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:35:35] Ep:6 [200/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.292 (0.529)  Prec@1 89.06 (81.70) Prec@5 100.00 (99.20)]\n",
      "*Train* [2022-11-03 07:35:55] Ep:6 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.359 (0.534)  Prec@1 85.00 (81.65) Prec@5 100.00 (99.10)]\n",
      "Ep:6 ends : loss=0.53, accuracy@1=81.65%, accuracy@5=99.10%\n",
      "*Train* [2022-11-03 07:35:55] Ep:7 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 0.599 (0.599)  Prec@1 78.12 (78.12) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:36:17] Ep:7 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.494 (0.508)  Prec@1 84.38 (81.98) Prec@5 98.44 (99.22)]\n",
      "*Train* [2022-11-03 07:36:38] Ep:7 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.550 (0.515)  Prec@1 80.00 (82.04) Prec@5 100.00 (99.20)]\n",
      "Ep:7 ends : loss=0.51, accuracy@1=82.04%, accuracy@5=99.20%\n",
      "*Train* [2022-11-03 07:36:39] Ep:8 [000/391] Time 0.27 (0.27) Data 0.15 (0.15) Base [Loss 0.668 (0.668)  Prec@1 73.44 (73.44) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:37:01] Ep:8 [200/391] Time 0.10 (0.11) Data 0.00 (0.00) Base [Loss 0.608 (0.510)  Prec@1 81.25 (82.63) Prec@5 98.44 (99.21)]\n",
      "*Train* [2022-11-03 07:37:22] Ep:8 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.541 (0.511)  Prec@1 77.50 (82.57) Prec@5 100.00 (99.17)]\n",
      "Ep:8 ends : loss=0.51, accuracy@1=82.57%, accuracy@5=99.17%\n",
      "*Train* [2022-11-03 07:37:22] Ep:9 [000/391] Time 0.28 (0.28) Data 0.16 (0.16) Base [Loss 0.605 (0.605)  Prec@1 78.12 (78.12) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:37:45] Ep:9 [200/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.522 (0.527)  Prec@1 84.38 (81.55) Prec@5 98.44 (99.14)]\n",
      "*Train* [2022-11-03 07:38:06] Ep:9 [390/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.729 (0.523)  Prec@1 77.50 (81.70) Prec@5 97.50 (99.12)]\n",
      "Ep:9 ends : loss=0.52, accuracy@1=81.70%, accuracy@5=99.12%\n",
      "Found best op for target cell:9\n",
      ": Structure(4 nodes with |skip_connect~0|+|skip_connect~0|nor_conv_3x3~1|+|none~0|avg_pool_3x3~1|avg_pool_3x3~2|) with accuracy=74.02%, confidence=64.105%, sensitivity=0.124, robustness=-0.077, step_sim=99.804\n",
      "\n",
      "\n",
      " Searching with a cell #10\n",
      "*Train* [2022-11-03 07:41:18] Ep:0 [000/391] Time 0.29 (0.29) Data 0.20 (0.20) Base [Loss 0.377 (0.377)  Prec@1 85.94 (85.94) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:41:41] Ep:0 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.528 (0.543)  Prec@1 79.69 (81.04) Prec@5 98.44 (99.12)]\n",
      "*Train* [2022-11-03 07:42:03] Ep:0 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.639 (0.535)  Prec@1 77.50 (81.41) Prec@5 100.00 (99.07)]\n",
      "Ep:0 ends : loss=0.53, accuracy@1=81.41%, accuracy@5=99.07%\n",
      "*Train* [2022-11-03 07:42:03] Ep:1 [000/391] Time 0.33 (0.33) Data 0.23 (0.23) Base [Loss 2.967 (2.967)  Prec@1 4.69 (4.69) Prec@5 43.75 (43.75)]\n",
      "*Train* [2022-11-03 07:42:25] Ep:1 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.625 (0.586)  Prec@1 75.00 (79.98) Prec@5 100.00 (98.73)]\n",
      "*Train* [2022-11-03 07:42:49] Ep:1 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.542 (0.564)  Prec@1 90.00 (80.60) Prec@5 100.00 (98.88)]\n",
      "Ep:1 ends : loss=0.56, accuracy@1=80.60%, accuracy@5=98.88%\n",
      "*Train* [2022-11-03 07:42:50] Ep:2 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 0.464 (0.464)  Prec@1 89.06 (89.06) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:43:14] Ep:2 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.420 (0.511)  Prec@1 81.25 (82.53) Prec@5 100.00 (99.12)]\n",
      "*Train* [2022-11-03 07:43:34] Ep:2 [390/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.556 (0.509)  Prec@1 87.50 (82.52) Prec@5 97.50 (99.16)]\n",
      "Ep:2 ends : loss=0.51, accuracy@1=82.52%, accuracy@5=99.16%\n",
      "*Train* [2022-11-03 07:43:35] Ep:3 [000/391] Time 0.29 (0.29) Data 0.16 (0.16) Base [Loss 0.646 (0.646)  Prec@1 79.69 (79.69) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:43:57] Ep:3 [200/391] Time 0.11 (0.12) Data 0.00 (0.00) Base [Loss 0.599 (0.521)  Prec@1 81.25 (82.28) Prec@5 100.00 (99.14)]\n",
      "*Train* [2022-11-03 07:44:18] Ep:3 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.727 (0.525)  Prec@1 75.00 (81.96) Prec@5 100.00 (99.16)]\n",
      "Ep:3 ends : loss=0.52, accuracy@1=81.96%, accuracy@5=99.16%\n",
      "*Train* [2022-11-03 07:44:18] Ep:4 [000/391] Time 0.29 (0.29) Data 0.20 (0.20) Base [Loss 2.425 (2.425)  Prec@1 18.75 (18.75) Prec@5 76.56 (76.56)]\n",
      "*Train* [2022-11-03 07:44:43] Ep:4 [200/391] Time 0.12 (0.13) Data 0.00 (0.00) Base [Loss 0.418 (0.590)  Prec@1 82.81 (79.98) Prec@5 100.00 (98.84)]\n",
      "*Train* [2022-11-03 07:45:04] Ep:4 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.572 (0.565)  Prec@1 80.00 (80.72) Prec@5 100.00 (98.99)]\n",
      "Ep:4 ends : loss=0.56, accuracy@1=80.72%, accuracy@5=98.99%\n",
      "*Train* [2022-11-03 07:45:04] Ep:5 [000/391] Time 0.29 (0.29) Data 0.18 (0.18) Base [Loss 0.784 (0.784)  Prec@1 75.00 (75.00) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:45:27] Ep:5 [200/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.448 (0.506)  Prec@1 82.81 (83.36) Prec@5 98.44 (99.29)]\n",
      "*Train* [2022-11-03 07:45:48] Ep:5 [390/391] Time 0.16 (0.11) Data 0.00 (0.00) Base [Loss 0.596 (0.511)  Prec@1 80.00 (82.65) Prec@5 100.00 (99.22)]\n",
      "Ep:5 ends : loss=0.51, accuracy@1=82.65%, accuracy@5=99.22%\n",
      "*Train* [2022-11-03 07:45:48] Ep:6 [000/391] Time 0.29 (0.29) Data 0.17 (0.17) Base [Loss 3.097 (3.097)  Prec@1 4.69 (4.69) Prec@5 53.12 (53.12)]\n",
      "*Train* [2022-11-03 07:46:12] Ep:6 [200/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.434 (0.567)  Prec@1 82.81 (80.56) Prec@5 100.00 (98.68)]\n",
      "*Train* [2022-11-03 07:46:33] Ep:6 [390/391] Time 0.14 (0.11) Data 0.00 (0.00) Base [Loss 0.473 (0.550)  Prec@1 82.50 (81.24) Prec@5 97.50 (98.84)]\n",
      "Ep:6 ends : loss=0.55, accuracy@1=81.24%, accuracy@5=98.84%\n",
      "*Train* [2022-11-03 07:46:33] Ep:7 [000/391] Time 0.29 (0.29) Data 0.17 (0.17) Base [Loss 0.848 (0.848)  Prec@1 70.31 (70.31) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:46:56] Ep:7 [200/391] Time 0.11 (0.11) Data 0.00 (0.00) Base [Loss 0.480 (0.509)  Prec@1 85.94 (82.60) Prec@5 100.00 (99.28)]\n",
      "*Train* [2022-11-03 07:47:20] Ep:7 [390/391] Time 0.21 (0.12) Data 0.00 (0.00) Base [Loss 0.304 (0.515)  Prec@1 90.00 (82.35) Prec@5 100.00 (99.19)]\n",
      "Ep:7 ends : loss=0.51, accuracy@1=82.35%, accuracy@5=99.19%\n",
      "*Train* [2022-11-03 07:47:20] Ep:8 [000/391] Time 0.29 (0.29) Data 0.20 (0.20) Base [Loss 0.434 (0.434)  Prec@1 85.94 (85.94) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:47:42] Ep:8 [200/391] Time 0.10 (0.11) Data 0.00 (0.00) Base [Loss 0.497 (0.478)  Prec@1 85.94 (84.01) Prec@5 100.00 (99.23)]\n",
      "*Train* [2022-11-03 07:48:04] Ep:8 [390/391] Time 0.22 (0.11) Data 0.00 (0.00) Base [Loss 0.601 (0.492)  Prec@1 77.50 (83.38) Prec@5 100.00 (99.22)]\n",
      "Ep:8 ends : loss=0.49, accuracy@1=83.38%, accuracy@5=99.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 07:48:04] Ep:9 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 0.409 (0.409)  Prec@1 85.94 (85.94) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:48:27] Ep:9 [200/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.404 (0.462)  Prec@1 84.38 (84.06) Prec@5 100.00 (99.34)]\n",
      "*Train* [2022-11-03 07:48:50] Ep:9 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.592 (0.480)  Prec@1 70.00 (83.32) Prec@5 100.00 (99.29)]\n",
      "Ep:9 ends : loss=0.48, accuracy@1=83.32%, accuracy@5=99.29%\n",
      "Found best op for target cell:10\n",
      ": Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|none~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_1x1~2|) with accuracy=76.95%, confidence=68.461%, sensitivity=0.154, robustness=-0.091, step_sim=99.761\n",
      "\n",
      "\n",
      " Searching with a cell #11\n",
      "*Train* [2022-11-03 07:51:50] Ep:0 [000/391] Time 0.29 (0.29) Data 0.19 (0.19) Base [Loss 0.553 (0.553)  Prec@1 81.25 (81.25) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:52:13] Ep:0 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.255 (0.529)  Prec@1 93.75 (81.92) Prec@5 100.00 (99.22)]\n",
      "*Train* [2022-11-03 07:52:37] Ep:0 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.391 (0.518)  Prec@1 90.00 (82.30) Prec@5 100.00 (99.26)]\n",
      "Ep:0 ends : loss=0.52, accuracy@1=82.30%, accuracy@5=99.26%\n",
      "*Train* [2022-11-03 07:52:37] Ep:1 [000/391] Time 0.31 (0.31) Data 0.20 (0.20) Base [Loss 2.537 (2.537)  Prec@1 21.88 (21.88) Prec@5 59.38 (59.38)]\n",
      "*Train* [2022-11-03 07:53:00] Ep:1 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.440 (0.571)  Prec@1 81.25 (80.64) Prec@5 100.00 (98.64)]\n",
      "*Train* [2022-11-03 07:53:23] Ep:1 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.538 (0.552)  Prec@1 82.50 (81.08) Prec@5 100.00 (98.88)]\n",
      "Ep:1 ends : loss=0.55, accuracy@1=81.08%, accuracy@5=98.88%\n",
      "*Train* [2022-11-03 07:53:23] Ep:2 [000/391] Time 0.27 (0.27) Data 0.17 (0.17) Base [Loss 0.682 (0.682)  Prec@1 67.19 (67.19) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:53:49] Ep:2 [200/391] Time 0.11 (0.13) Data 0.00 (0.00) Base [Loss 0.399 (0.499)  Prec@1 87.50 (82.80) Prec@5 98.44 (99.27)]\n",
      "*Train* [2022-11-03 07:54:11] Ep:2 [390/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.766 (0.498)  Prec@1 75.00 (82.92) Prec@5 95.00 (99.22)]\n",
      "Ep:2 ends : loss=0.50, accuracy@1=82.92%, accuracy@5=99.22%\n",
      "*Train* [2022-11-03 07:54:11] Ep:3 [000/391] Time 0.27 (0.27) Data 0.17 (0.17) Base [Loss 0.504 (0.504)  Prec@1 79.69 (79.69) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:54:36] Ep:3 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.545 (0.507)  Prec@1 79.69 (82.75) Prec@5 100.00 (99.22)]\n",
      "*Train* [2022-11-03 07:54:59] Ep:3 [390/391] Time 0.15 (0.12) Data 0.00 (0.00) Base [Loss 0.583 (0.502)  Prec@1 75.00 (82.63) Prec@5 100.00 (99.23)]\n",
      "Ep:3 ends : loss=0.50, accuracy@1=82.63%, accuracy@5=99.23%\n",
      "*Train* [2022-11-03 07:54:59] Ep:4 [000/391] Time 0.29 (0.29) Data 0.18 (0.18) Base [Loss 0.537 (0.537)  Prec@1 85.94 (85.94) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:55:23] Ep:4 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.661 (0.506)  Prec@1 79.69 (82.77) Prec@5 98.44 (99.14)]\n",
      "*Train* [2022-11-03 07:55:46] Ep:4 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.683 (0.511)  Prec@1 72.50 (82.58) Prec@5 100.00 (99.12)]\n",
      "Ep:4 ends : loss=0.51, accuracy@1=82.58%, accuracy@5=99.12%\n",
      "*Train* [2022-11-03 07:55:46] Ep:5 [000/391] Time 0.33 (0.33) Data 0.21 (0.21) Base [Loss 0.531 (0.531)  Prec@1 81.25 (81.25) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:56:09] Ep:5 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.463 (0.474)  Prec@1 82.81 (84.17) Prec@5 100.00 (99.18)]\n",
      "*Train* [2022-11-03 07:56:31] Ep:5 [390/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.353 (0.479)  Prec@1 87.50 (83.72) Prec@5 100.00 (99.30)]\n",
      "Ep:5 ends : loss=0.48, accuracy@1=83.72%, accuracy@5=99.30%\n",
      "*Train* [2022-11-03 07:56:32] Ep:6 [000/391] Time 0.27 (0.27) Data 0.16 (0.16) Base [Loss 0.457 (0.457)  Prec@1 85.94 (85.94) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 07:56:57] Ep:6 [200/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.576 (0.470)  Prec@1 79.69 (83.82) Prec@5 96.88 (99.38)]\n",
      "*Train* [2022-11-03 07:57:17] Ep:6 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.659 (0.479)  Prec@1 75.00 (83.58) Prec@5 100.00 (99.28)]\n",
      "Ep:6 ends : loss=0.48, accuracy@1=83.58%, accuracy@5=99.28%\n",
      "*Train* [2022-11-03 07:57:17] Ep:7 [000/391] Time 0.30 (0.30) Data 0.21 (0.21) Base [Loss 0.646 (0.646)  Prec@1 76.56 (76.56) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 07:57:41] Ep:7 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.548 (0.473)  Prec@1 81.25 (83.64) Prec@5 98.44 (99.34)]\n",
      "*Train* [2022-11-03 07:58:03] Ep:7 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.257 (0.478)  Prec@1 92.50 (83.59) Prec@5 100.00 (99.36)]\n",
      "Ep:7 ends : loss=0.48, accuracy@1=83.59%, accuracy@5=99.36%\n",
      "*Train* [2022-11-03 07:58:03] Ep:8 [000/391] Time 0.27 (0.27) Data 0.15 (0.15) Base [Loss 0.953 (0.953)  Prec@1 71.88 (71.88) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 07:58:27] Ep:8 [200/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.527 (0.512)  Prec@1 81.25 (82.73) Prec@5 98.44 (99.30)]\n",
      "*Train* [2022-11-03 07:58:47] Ep:8 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.577 (0.514)  Prec@1 77.50 (82.36) Prec@5 100.00 (99.21)]\n",
      "Ep:8 ends : loss=0.51, accuracy@1=82.36%, accuracy@5=99.21%\n",
      "*Train* [2022-11-03 07:58:48] Ep:9 [000/391] Time 0.25 (0.25) Data 0.15 (0.15) Base [Loss 0.572 (0.572)  Prec@1 81.25 (81.25) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 07:59:11] Ep:9 [200/391] Time 0.21 (0.12) Data 0.00 (0.00) Base [Loss 0.489 (0.477)  Prec@1 84.38 (83.39) Prec@5 100.00 (99.35)]\n",
      "*Train* [2022-11-03 07:59:35] Ep:9 [390/391] Time 0.22 (0.12) Data 0.00 (0.00) Base [Loss 0.553 (0.478)  Prec@1 80.00 (83.49) Prec@5 100.00 (99.32)]\n",
      "Ep:9 ends : loss=0.48, accuracy@1=83.49%, accuracy@5=99.32%\n",
      "Found best op for target cell:11\n",
      ": Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_1x1~0|none~1|+|avg_pool_3x3~0|nor_conv_3x3~1|avg_pool_3x3~2|) with accuracy=75.78%, confidence=67.913%, sensitivity=0.145, robustness=-0.070, step_sim=99.773\n",
      "\n",
      "\n",
      " Searching with a cell #12\n",
      "*Train* [2022-11-03 08:02:42] Ep:0 [000/391] Time 0.33 (0.33) Data 0.23 (0.23) Base [Loss 0.494 (0.494)  Prec@1 87.50 (87.50) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 08:03:08] Ep:0 [200/391] Time 0.14 (0.13) Data 0.00 (0.00) Base [Loss 0.329 (0.487)  Prec@1 89.06 (83.22) Prec@5 100.00 (99.25)]\n",
      "*Train* [2022-11-03 08:03:31] Ep:0 [390/391] Time 0.16 (0.13) Data 0.00 (0.00) Base [Loss 0.318 (0.491)  Prec@1 87.50 (83.07) Prec@5 100.00 (99.26)]\n",
      "Ep:0 ends : loss=0.49, accuracy@1=83.07%, accuracy@5=99.26%\n",
      "*Train* [2022-11-03 08:03:31] Ep:1 [000/391] Time 0.33 (0.33) Data 0.16 (0.16) Base [Loss 0.299 (0.299)  Prec@1 89.06 (89.06) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:03:57] Ep:1 [200/391] Time 0.11 (0.13) Data 0.00 (0.00) Base [Loss 0.418 (0.472)  Prec@1 84.38 (83.63) Prec@5 100.00 (99.21)]\n",
      "*Train* [2022-11-03 08:04:20] Ep:1 [390/391] Time 0.08 (0.13) Data 0.00 (0.00) Base [Loss 0.335 (0.479)  Prec@1 87.50 (83.43) Prec@5 100.00 (99.23)]\n",
      "Ep:1 ends : loss=0.48, accuracy@1=83.43%, accuracy@5=99.23%\n",
      "*Train* [2022-11-03 08:04:21] Ep:2 [000/391] Time 0.36 (0.36) Data 0.20 (0.20) Base [Loss 0.568 (0.568)  Prec@1 81.25 (81.25) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 08:04:46] Ep:2 [200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.419 (0.495)  Prec@1 89.06 (83.11) Prec@5 98.44 (99.14)]\n",
      "*Train* [2022-11-03 08:05:09] Ep:2 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.466 (0.491)  Prec@1 85.00 (83.30) Prec@5 100.00 (99.22)]\n",
      "Ep:2 ends : loss=0.49, accuracy@1=83.30%, accuracy@5=99.22%\n",
      "*Train* [2022-11-03 08:05:09] Ep:3 [000/391] Time 0.31 (0.31) Data 0.16 (0.16) Base [Loss 0.291 (0.291)  Prec@1 95.31 (95.31) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:05:34] Ep:3 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.568 (0.464)  Prec@1 78.12 (83.96) Prec@5 98.44 (99.32)]\n",
      "*Train* [2022-11-03 08:05:56] Ep:3 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.559 (0.471)  Prec@1 70.00 (83.64) Prec@5 100.00 (99.30)]\n",
      "Ep:3 ends : loss=0.47, accuracy@1=83.64%, accuracy@5=99.30%\n",
      "*Train* [2022-11-03 08:05:56] Ep:4 [000/391] Time 0.27 (0.27) Data 0.18 (0.18) Base [Loss 0.461 (0.461)  Prec@1 81.25 (81.25) Prec@5 100.00 (100.00)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Train* [2022-11-03 08:06:21] Ep:4 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.531 (0.453)  Prec@1 84.38 (84.60) Prec@5 100.00 (99.28)]\n",
      "*Train* [2022-11-03 08:06:44] Ep:4 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.574 (0.462)  Prec@1 82.50 (84.24) Prec@5 100.00 (99.31)]\n",
      "Ep:4 ends : loss=0.46, accuracy@1=84.24%, accuracy@5=99.31%\n",
      "*Train* [2022-11-03 08:06:44] Ep:5 [000/391] Time 0.39 (0.39) Data 0.20 (0.20) Base [Loss 0.531 (0.531)  Prec@1 84.38 (84.38) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 08:07:09] Ep:5 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.414 (0.468)  Prec@1 81.25 (84.13) Prec@5 100.00 (99.19)]\n",
      "*Train* [2022-11-03 08:07:29] Ep:5 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.594 (0.471)  Prec@1 80.00 (83.83) Prec@5 100.00 (99.23)]\n",
      "Ep:5 ends : loss=0.47, accuracy@1=83.83%, accuracy@5=99.23%\n",
      "*Train* [2022-11-03 08:07:30] Ep:6 [000/391] Time 0.28 (0.28) Data 0.18 (0.18) Base [Loss 1.594 (1.594)  Prec@1 40.62 (40.62) Prec@5 95.31 (95.31)]\n",
      "*Train* [2022-11-03 08:07:55] Ep:6 [200/391] Time 0.15 (0.13) Data 0.00 (0.00) Base [Loss 0.462 (0.516)  Prec@1 82.81 (82.45) Prec@5 100.00 (99.23)]\n",
      "*Train* [2022-11-03 08:08:18] Ep:6 [390/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.781 (0.516)  Prec@1 75.00 (82.42) Prec@5 97.50 (99.18)]\n",
      "Ep:6 ends : loss=0.52, accuracy@1=82.42%, accuracy@5=99.18%\n",
      "*Train* [2022-11-03 08:08:18] Ep:7 [000/391] Time 0.25 (0.25) Data 0.15 (0.15) Base [Loss 0.515 (0.515)  Prec@1 81.25 (81.25) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:08:42] Ep:7 [200/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.375 (0.449)  Prec@1 89.06 (84.58) Prec@5 96.88 (99.42)]\n",
      "*Train* [2022-11-03 08:09:05] Ep:7 [390/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.527 (0.465)  Prec@1 80.00 (83.89) Prec@5 100.00 (99.31)]\n",
      "Ep:7 ends : loss=0.47, accuracy@1=83.89%, accuracy@5=99.31%\n",
      "*Train* [2022-11-03 08:09:05] Ep:8 [000/391] Time 0.28 (0.28) Data 0.18 (0.18) Base [Loss 0.357 (0.357)  Prec@1 90.62 (90.62) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:09:29] Ep:8 [200/391] Time 0.08 (0.12) Data 0.00 (0.00) Base [Loss 0.467 (0.458)  Prec@1 81.25 (84.38) Prec@5 98.44 (99.28)]\n",
      "*Train* [2022-11-03 08:09:53] Ep:8 [390/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.450 (0.467)  Prec@1 82.50 (84.16) Prec@5 100.00 (99.29)]\n",
      "Ep:8 ends : loss=0.47, accuracy@1=84.16%, accuracy@5=99.29%\n",
      "*Train* [2022-11-03 08:09:53] Ep:9 [000/391] Time 0.29 (0.29) Data 0.18 (0.18) Base [Loss 0.440 (0.440)  Prec@1 87.50 (87.50) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 08:10:15] Ep:9 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.425 (0.447)  Prec@1 87.50 (84.90) Prec@5 98.44 (99.39)]\n",
      "*Train* [2022-11-03 08:10:35] Ep:9 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.305 (0.449)  Prec@1 92.50 (84.58) Prec@5 100.00 (99.36)]\n",
      "Ep:9 ends : loss=0.45, accuracy@1=84.58%, accuracy@5=99.36%\n",
      "Found best op for target cell:12\n",
      ": Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|avg_pool_3x3~0|skip_connect~1|nor_conv_1x1~2|) with accuracy=79.88%, confidence=72.596%, sensitivity=0.120, robustness=-0.074, step_sim=99.834\n",
      "\n",
      "\n",
      " Searching with a cell #13\n",
      "*Train* [2022-11-03 08:13:34] Ep:0 [000/391] Time 0.33 (0.33) Data 0.17 (0.17) Base [Loss 3.008 (3.008)  Prec@1 4.69 (4.69) Prec@5 35.94 (35.94)]\n",
      "*Train* [2022-11-03 08:13:58] Ep:0 [200/391] Time 0.15 (0.12) Data 0.00 (0.00) Base [Loss 0.453 (0.519)  Prec@1 84.38 (82.14) Prec@5 100.00 (98.92)]\n",
      "*Train* [2022-11-03 08:14:22] Ep:0 [390/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.743 (0.510)  Prec@1 77.50 (82.52) Prec@5 100.00 (99.05)]\n",
      "Ep:0 ends : loss=0.51, accuracy@1=82.52%, accuracy@5=99.05%\n",
      "*Train* [2022-11-03 08:14:22] Ep:1 [000/391] Time 0.35 (0.35) Data 0.21 (0.21) Base [Loss 2.780 (2.780)  Prec@1 10.94 (10.94) Prec@5 46.88 (46.88)]\n",
      "*Train* [2022-11-03 08:14:47] Ep:1 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.664 (0.551)  Prec@1 78.12 (81.73) Prec@5 98.44 (98.68)]\n",
      "*Train* [2022-11-03 08:15:10] Ep:1 [390/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.572 (0.539)  Prec@1 82.50 (82.02) Prec@5 97.50 (98.93)]\n",
      "Ep:1 ends : loss=0.54, accuracy@1=82.02%, accuracy@5=98.93%\n",
      "*Train* [2022-11-03 08:15:10] Ep:2 [000/391] Time 0.28 (0.28) Data 0.19 (0.19) Base [Loss 3.159 (3.159)  Prec@1 6.25 (6.25) Prec@5 59.38 (59.38)]\n",
      "*Train* [2022-11-03 08:15:33] Ep:2 [200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.528 (0.527)  Prec@1 76.56 (82.18) Prec@5 100.00 (98.89)]\n",
      "*Train* [2022-11-03 08:15:57] Ep:2 [390/391] Time 0.24 (0.12) Data 0.00 (0.00) Base [Loss 0.449 (0.509)  Prec@1 82.50 (82.54) Prec@5 100.00 (99.01)]\n",
      "Ep:2 ends : loss=0.51, accuracy@1=82.54%, accuracy@5=99.01%\n",
      "*Train* [2022-11-03 08:15:57] Ep:3 [000/391] Time 0.30 (0.30) Data 0.19 (0.19) Base [Loss 0.497 (0.497)  Prec@1 79.69 (79.69) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:16:21] Ep:3 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 0.368 (0.450)  Prec@1 84.38 (84.48) Prec@5 100.00 (99.46)]\n",
      "*Train* [2022-11-03 08:16:42] Ep:3 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 0.384 (0.452)  Prec@1 92.50 (84.38) Prec@5 100.00 (99.39)]\n",
      "Ep:3 ends : loss=0.45, accuracy@1=84.38%, accuracy@5=99.39%\n",
      "*Train* [2022-11-03 08:16:43] Ep:4 [000/391] Time 0.26 (0.26) Data 0.17 (0.17) Base [Loss 0.588 (0.588)  Prec@1 78.12 (78.12) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:17:08] Ep:4 [200/391] Time 0.08 (0.13) Data 0.00 (0.00) Base [Loss 0.423 (0.447)  Prec@1 84.38 (84.65) Prec@5 98.44 (99.35)]\n",
      "*Train* [2022-11-03 08:17:32] Ep:4 [390/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.299 (0.448)  Prec@1 90.00 (84.62) Prec@5 100.00 (99.40)]\n",
      "Ep:4 ends : loss=0.45, accuracy@1=84.62%, accuracy@5=99.40%\n",
      "*Train* [2022-11-03 08:17:32] Ep:5 [000/391] Time 0.32 (0.32) Data 0.20 (0.20) Base [Loss 0.518 (0.518)  Prec@1 84.38 (84.38) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 08:17:55] Ep:5 [200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.498 (0.433)  Prec@1 82.81 (84.84) Prec@5 100.00 (99.41)]\n",
      "*Train* [2022-11-03 08:18:16] Ep:5 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.737 (0.443)  Prec@1 75.00 (84.65) Prec@5 100.00 (99.36)]\n",
      "Ep:5 ends : loss=0.44, accuracy@1=84.65%, accuracy@5=99.36%\n",
      "*Train* [2022-11-03 08:18:16] Ep:6 [000/391] Time 0.33 (0.33) Data 0.19 (0.19) Base [Loss 0.457 (0.457)  Prec@1 82.81 (82.81) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:18:39] Ep:6 [200/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.407 (0.434)  Prec@1 82.81 (84.77) Prec@5 100.00 (99.46)]\n",
      "*Train* [2022-11-03 08:19:00] Ep:6 [390/391] Time 0.13 (0.11) Data 0.00 (0.00) Base [Loss 0.575 (0.446)  Prec@1 80.00 (84.49) Prec@5 100.00 (99.34)]\n",
      "Ep:6 ends : loss=0.45, accuracy@1=84.49%, accuracy@5=99.34%\n",
      "*Train* [2022-11-03 08:19:00] Ep:7 [000/391] Time 0.36 (0.36) Data 0.17 (0.17) Base [Loss 0.497 (0.497)  Prec@1 82.81 (82.81) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 08:19:25] Ep:7 [200/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.431 (0.492)  Prec@1 85.94 (82.98) Prec@5 100.00 (99.13)]\n",
      "*Train* [2022-11-03 08:19:49] Ep:7 [390/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.589 (0.495)  Prec@1 77.50 (82.74) Prec@5 100.00 (99.15)]\n",
      "Ep:7 ends : loss=0.49, accuracy@1=82.74%, accuracy@5=99.15%\n",
      "*Train* [2022-11-03 08:19:49] Ep:8 [000/391] Time 0.25 (0.25) Data 0.15 (0.15) Base [Loss 0.639 (0.639)  Prec@1 73.44 (73.44) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:20:13] Ep:8 [200/391] Time 0.14 (0.12) Data 0.00 (0.00) Base [Loss 0.386 (0.424)  Prec@1 85.94 (85.52) Prec@5 100.00 (99.40)]\n",
      "*Train* [2022-11-03 08:20:37] Ep:8 [390/391] Time 0.15 (0.12) Data 0.00 (0.00) Base [Loss 0.324 (0.434)  Prec@1 92.50 (84.94) Prec@5 97.50 (99.38)]\n",
      "Ep:8 ends : loss=0.43, accuracy@1=84.94%, accuracy@5=99.38%\n",
      "*Train* [2022-11-03 08:20:38] Ep:9 [000/391] Time 0.35 (0.35) Data 0.15 (0.15) Base [Loss 0.632 (0.632)  Prec@1 78.12 (78.12) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 08:21:00] Ep:9 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.517 (0.473)  Prec@1 82.81 (84.01) Prec@5 100.00 (99.39)]\n",
      "*Train* [2022-11-03 08:21:21] Ep:9 [390/391] Time 0.22 (0.11) Data 0.00 (0.00) Base [Loss 0.536 (0.468)  Prec@1 82.50 (83.99) Prec@5 100.00 (99.36)]\n",
      "Ep:9 ends : loss=0.47, accuracy@1=83.99%, accuracy@5=99.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best op for target cell:13\n",
      ": Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_1x1~0|nor_conv_3x3~1|skip_connect~2|) with accuracy=76.95%, confidence=68.852%, sensitivity=0.159, robustness=-0.080, step_sim=99.787\n",
      "\n",
      "\n",
      " Searching with a cell #14\n",
      "*Train* [2022-11-03 08:24:39] Ep:0 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 0.674 (0.674)  Prec@1 76.56 (76.56) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:25:04] Ep:0 [200/391] Time 0.12 (0.13) Data 0.00 (0.00) Base [Loss 0.548 (0.435)  Prec@1 79.69 (85.30) Prec@5 98.44 (99.29)]\n",
      "*Train* [2022-11-03 08:25:30] Ep:0 [390/391] Time 0.15 (0.13) Data 0.00 (0.00) Base [Loss 0.610 (0.437)  Prec@1 77.50 (85.27) Prec@5 100.00 (99.31)]\n",
      "Ep:0 ends : loss=0.44, accuracy@1=85.27%, accuracy@5=99.31%\n",
      "*Train* [2022-11-03 08:25:30] Ep:1 [000/391] Time 0.29 (0.29) Data 0.19 (0.19) Base [Loss 2.821 (2.821)  Prec@1 4.69 (4.69) Prec@5 50.00 (50.00)]\n",
      "*Train* [2022-11-03 08:25:57] Ep:1 [200/391] Time 0.18 (0.14) Data 0.00 (0.00) Base [Loss 0.368 (0.515)  Prec@1 84.38 (82.52) Prec@5 100.00 (98.98)]\n",
      "*Train* [2022-11-03 08:26:24] Ep:1 [390/391] Time 0.23 (0.14) Data 0.00 (0.00) Base [Loss 0.613 (0.502)  Prec@1 82.50 (82.77) Prec@5 97.50 (99.12)]\n",
      "Ep:1 ends : loss=0.50, accuracy@1=82.77%, accuracy@5=99.12%\n",
      "*Train* [2022-11-03 08:26:24] Ep:2 [000/391] Time 0.36 (0.36) Data 0.20 (0.20) Base [Loss 0.393 (0.393)  Prec@1 89.06 (89.06) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:26:48] Ep:2 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 0.437 (0.428)  Prec@1 87.50 (85.52) Prec@5 98.44 (99.42)]\n",
      "*Train* [2022-11-03 08:27:13] Ep:2 [390/391] Time 0.14 (0.13) Data 0.00 (0.00) Base [Loss 0.391 (0.442)  Prec@1 85.00 (84.96) Prec@5 100.00 (99.40)]\n",
      "Ep:2 ends : loss=0.44, accuracy@1=84.96%, accuracy@5=99.40%\n",
      "*Train* [2022-11-03 08:27:13] Ep:3 [000/391] Time 0.29 (0.29) Data 0.18 (0.18) Base [Loss 0.424 (0.424)  Prec@1 79.69 (79.69) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:27:39] Ep:3 [200/391] Time 0.10 (0.13) Data 0.00 (0.00) Base [Loss 0.331 (0.410)  Prec@1 87.50 (86.03) Prec@5 100.00 (99.45)]\n",
      "*Train* [2022-11-03 08:28:03] Ep:3 [390/391] Time 0.08 (0.13) Data 0.00 (0.00) Base [Loss 0.345 (0.426)  Prec@1 90.00 (85.36) Prec@5 100.00 (99.45)]\n",
      "Ep:3 ends : loss=0.43, accuracy@1=85.36%, accuracy@5=99.45%\n",
      "*Train* [2022-11-03 08:28:04] Ep:4 [000/391] Time 0.38 (0.38) Data 0.17 (0.17) Base [Loss 0.395 (0.395)  Prec@1 81.25 (81.25) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-03 08:28:31] Ep:4 [200/391] Time 0.14 (0.14) Data 0.00 (0.00) Base [Loss 0.387 (0.430)  Prec@1 87.50 (85.44) Prec@5 98.44 (99.33)]\n",
      "*Train* [2022-11-03 08:28:56] Ep:4 [390/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.333 (0.431)  Prec@1 87.50 (85.19) Prec@5 100.00 (99.37)]\n",
      "Ep:4 ends : loss=0.43, accuracy@1=85.19%, accuracy@5=99.37%\n",
      "*Train* [2022-11-03 08:28:56] Ep:5 [000/391] Time 0.29 (0.29) Data 0.19 (0.19) Base [Loss 0.313 (0.313)  Prec@1 81.25 (81.25) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:29:22] Ep:5 [200/391] Time 0.12 (0.13) Data 0.00 (0.00) Base [Loss 0.481 (0.416)  Prec@1 82.81 (85.95) Prec@5 100.00 (99.49)]\n",
      "*Train* [2022-11-03 08:29:48] Ep:5 [390/391] Time 0.10 (0.13) Data 0.00 (0.00) Base [Loss 0.357 (0.422)  Prec@1 90.00 (85.72) Prec@5 100.00 (99.46)]\n",
      "Ep:5 ends : loss=0.42, accuracy@1=85.72%, accuracy@5=99.46%\n",
      "*Train* [2022-11-03 08:29:48] Ep:6 [000/391] Time 0.30 (0.30) Data 0.20 (0.20) Base [Loss 0.241 (0.241)  Prec@1 92.19 (92.19) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:30:14] Ep:6 [200/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.764 (0.420)  Prec@1 76.56 (85.32) Prec@5 100.00 (99.47)]\n",
      "*Train* [2022-11-03 08:30:38] Ep:6 [390/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.174 (0.428)  Prec@1 95.00 (85.19) Prec@5 100.00 (99.42)]\n",
      "Ep:6 ends : loss=0.43, accuracy@1=85.19%, accuracy@5=99.42%\n",
      "*Train* [2022-11-03 08:30:39] Ep:7 [000/391] Time 0.49 (0.49) Data 0.18 (0.18) Base [Loss 3.279 (3.279)  Prec@1 4.69 (4.69) Prec@5 43.75 (43.75)]\n",
      "*Train* [2022-11-03 08:31:05] Ep:7 [200/391] Time 0.15 (0.13) Data 0.00 (0.00) Base [Loss 0.599 (0.518)  Prec@1 76.56 (82.34) Prec@5 98.44 (98.88)]\n",
      "*Train* [2022-11-03 08:31:29] Ep:7 [390/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.945 (0.491)  Prec@1 67.50 (83.21) Prec@5 97.50 (99.18)]\n",
      "Ep:7 ends : loss=0.49, accuracy@1=83.21%, accuracy@5=99.18%\n",
      "*Train* [2022-11-03 08:31:30] Ep:8 [000/391] Time 0.30 (0.30) Data 0.19 (0.19) Base [Loss 1.201 (1.201)  Prec@1 60.94 (60.94) Prec@5 96.88 (96.88)]\n",
      "*Train* [2022-11-03 08:31:56] Ep:8 [200/391] Time 0.13 (0.13) Data 0.00 (0.00) Base [Loss 0.481 (0.504)  Prec@1 85.94 (82.66) Prec@5 96.88 (99.18)]\n",
      "*Train* [2022-11-03 08:32:21] Ep:8 [390/391] Time 0.10 (0.13) Data 0.00 (0.00) Base [Loss 0.459 (0.488)  Prec@1 77.50 (83.24) Prec@5 100.00 (99.20)]\n",
      "Ep:8 ends : loss=0.49, accuracy@1=83.24%, accuracy@5=99.20%\n",
      "*Train* [2022-11-03 08:32:21] Ep:9 [000/391] Time 0.39 (0.39) Data 0.21 (0.21) Base [Loss 0.387 (0.387)  Prec@1 90.62 (90.62) Prec@5 100.00 (100.00)]\n",
      "*Train* [2022-11-03 08:32:48] Ep:9 [200/391] Time 0.11 (0.14) Data 0.00 (0.00) Base [Loss 0.534 (0.412)  Prec@1 82.81 (86.25) Prec@5 100.00 (99.54)]\n",
      "*Train* [2022-11-03 08:33:12] Ep:9 [390/391] Time 0.15 (0.13) Data 0.00 (0.00) Base [Loss 0.500 (0.416)  Prec@1 85.00 (85.88) Prec@5 100.00 (99.46)]\n",
      "Ep:9 ends : loss=0.42, accuracy@1=85.88%, accuracy@5=99.46%\n",
      "Found best op for target cell:14\n",
      ": Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|none~0|skip_connect~1|nor_conv_3x3~2|) with accuracy=77.73%, confidence=70.848%, sensitivity=0.173, robustness=-0.098, step_sim=99.647\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|none~0|nor_conv_3x3~1|none~2|)\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_1x1~1|none~2|)\n",
      "Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|none~1|+|skip_connect~0|none~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|none~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|nor_conv_3x3~0|avg_pool_3x3~1|nor_conv_1x1~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_3x3~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|nor_conv_1x1~0|avg_pool_3x3~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|avg_pool_3x3~0|none~1|+|nor_conv_3x3~0|avg_pool_3x3~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|nor_conv_1x1~0|skip_connect~1|+|nor_conv_3x3~0|avg_pool_3x3~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|skip_connect~0|nor_conv_3x3~1|+|none~0|avg_pool_3x3~1|avg_pool_3x3~2|)\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|none~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_1x1~2|)\n",
      "Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_1x1~0|none~1|+|avg_pool_3x3~0|nor_conv_3x3~1|avg_pool_3x3~2|)\n",
      "Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|avg_pool_3x3~0|skip_connect~1|nor_conv_1x1~2|)\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_1x1~0|nor_conv_3x3~1|skip_connect~2|)\n",
      "Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|none~0|skip_connect~1|nor_conv_3x3~2|)\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "\n",
    "################# initialize\n",
    "cells = []\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        cells.append(m)\n",
    "num_cells = len(cells)\n",
    "print(\"total number of nodes:{}\".format(num_cells*xargs.max_nodes))\n",
    "        \n",
    "op_names = deepcopy(cells[0].op_names)\n",
    "op_names_wo_none = deepcopy(op_names)\n",
    "if \"none\" in op_names_wo_none:\n",
    "    op_names_wo_none.remove(\"none\")\n",
    "\n",
    "genotypes = []\n",
    "for i in range(1, xargs.max_nodes):\n",
    "    xlist = []\n",
    "    for j in range(i):\n",
    "        node_str = \"{:}<-{:}\".format(i, j)\n",
    "        if i-j==1:\n",
    "            op_name = \"skip_connect\"\n",
    "        else:\n",
    "            op_name = \"none\"\n",
    "        xlist.append((op_name, j))\n",
    "    genotypes.append(tuple(xlist))\n",
    "init_arch = Structure(genotypes)\n",
    "\n",
    "for c in cells:\n",
    "    c.arch_cache = init_arch\n",
    "\n",
    "### gen possible connections of a target node\n",
    "possible_connections = {}\n",
    "for target_node_idx in range(1,xargs.max_nodes):\n",
    "    possible_connections[target_node_idx] = list()\n",
    "    xlists = []\n",
    "    for src_node in range(target_node_idx):\n",
    "        node_str = \"{:}<-{:}\".format(target_node_idx, src_node)\n",
    "        # select possible ops\n",
    "#         if target_node_idx - src_node == 1:\n",
    "#             op_names_tmp = op_names_wo_none\n",
    "#         else:\n",
    "#             op_names_tmp = op_names\n",
    "        op_names_tmp = op_names\n",
    "            \n",
    "        if len(xlists) == 0: # initial iteration\n",
    "            for op_name in op_names_tmp:\n",
    "                xlists.append([(op_name, src_node)])\n",
    "        else:\n",
    "            new_xlists = []\n",
    "            for op_name in op_names_tmp:\n",
    "                for xlist in xlists:\n",
    "                    new_xlist = deepcopy(xlist)\n",
    "                    new_xlist.append((op_name, src_node))\n",
    "                    new_xlists.append(new_xlist)\n",
    "            xlists = new_xlists\n",
    "    for xlist in xlists:\n",
    "        selected_ops = []\n",
    "        for l in xlist:\n",
    "            selected_ops.append(l[0])\n",
    "        if sum(np.array(selected_ops) == \"none\") == len(selected_ops):\n",
    "            continue\n",
    "        possible_connections[target_node_idx].append(tuple(xlist))\n",
    "    print(\"target_node:{}\".format(target_node_idx), len(possible_connections[target_node_idx]))\n",
    "        \n",
    "### train while generating random architectures by mutating connections of a target node\n",
    "\n",
    "for arch_loop in range(1):\n",
    "    for target_cell_idx in range(num_cells):\n",
    "        target_cell = cells[target_cell_idx]\n",
    "        print(\"\\n\\n Searching with a cell #{}\".format(target_cell_idx))\n",
    "        ####\n",
    "        for m in target_cell.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "        ####\n",
    "        ## training\n",
    "        for ep in range(10):\n",
    "            ###\n",
    "            genotypes = []\n",
    "            for n in range(1, xargs.max_nodes):\n",
    "                genotypes.append(random.choice(possible_connections[n]))\n",
    "            arch = Structure(genotypes)\n",
    "            target_cell.arch_cache = arch\n",
    "#             arch = target_cell.random_genotype(True)\n",
    "            ###\n",
    "            data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "            base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "            network.train()\n",
    "            end = time.time()\n",
    "            print_freq = 200\n",
    "            for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(search_loader):\n",
    "                ######### forward/backward/optim\n",
    "                base_targets = base_targets.cuda(non_blocking=True)\n",
    "                arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "                # measure data loading time\n",
    "                data_time.update(time.time() - end)\n",
    "                w_optimizer.zero_grad()\n",
    "                _, logits = network(base_inputs)\n",
    "                base_loss = criterion(logits, base_targets)\n",
    "                base_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "                w_optimizer.step()\n",
    "\n",
    "                ######### logging\n",
    "                base_prec1, base_prec5 = obtain_accuracy(logits.data, base_targets.data, topk=(1, 5))\n",
    "                base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "                base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "                base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "                if step % print_freq == 0 or step + 1 == len(search_loader):\n",
    "                    Sstr = (\"*Train* \"+ time_string()+\" Ep:{:} [{:03d}/{:03d}]\".format(ep, step, len(search_loader)))\n",
    "                    Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(batch_time=batch_time, data_time=data_time)\n",
    "                    Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(loss=base_losses, top1=base_top1, top5=base_top5)\n",
    "                    logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "\n",
    "            logger.log(\"Ep:{:} ends : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(ep, base_losses.avg, base_top1.avg, base_top5.avg))\n",
    "        ## evaluation\n",
    "        network.train()\n",
    "        archs, metric_accs, metric_confidences, metric_sensitivities, metric_robustnesses, metric_step_sims = [], [], [], [], [], []\n",
    "        loader_iter = iter(valid_loader)\n",
    "        for search_iter in range(200):\n",
    "            ###### random gen\n",
    "            genotypes = []\n",
    "            for n in range(1, xargs.max_nodes):\n",
    "                genotypes.append(random.choice(possible_connections[n]))\n",
    "            arch = Structure(genotypes)\n",
    "            target_cell.arch_cache = arch\n",
    "#             arch = target_cell.random_genotype(True)\n",
    "            ###### measure metrics\n",
    "            try:\n",
    "                inputs, targets = next(loader_iter)\n",
    "            except:\n",
    "                loader_iter = iter(valid_loader)\n",
    "                inputs, targets = next(loader_iter)\n",
    "            inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            valid_acc, confidence, sensitivity, robustness = acc_confidence_robustness_metrics(network, inputs, targets)\n",
    "            step_sim = step_sim_metric(network, criterion, inputs, targets)\n",
    "            archs.append(arch)\n",
    "            metric_accs.append(valid_acc)\n",
    "            metric_confidences.append(confidence)\n",
    "            metric_sensitivities.append(sensitivity)\n",
    "            metric_robustnesses.append(robustness)\n",
    "            metric_step_sims.append(step_sim)\n",
    "        rank_accs, rank_confidences, rank_sensitivities, rank_robustnesses, rank_step_sims = stats.rankdata(metric_accs), stats.rankdata(metric_confidences), stats.rankdata(metric_sensitivities), stats.rankdata(metric_robustnesses), stats.rankdata(metric_step_sims)\n",
    "        l = len(rank_accs)\n",
    "        rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_robustnesses/l)+np.log(rank_step_sims/l)\n",
    "#             rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_step_sims/l)\n",
    "        best_idx = np.argmax(rank_agg)\n",
    "        best_arch, best_acc, best_conf, best_sensitivity, best_robust, best_step_sim = archs[best_idx], metric_accs[best_idx], metric_confidences[best_idx], metric_sensitivities[best_idx], metric_robustnesses[best_idx], metric_step_sims[best_idx]\n",
    "        logger.log(\"Found best op for target cell:{}\".format(target_cell_idx))\n",
    "        logger.log(\": {:} with accuracy={:.2f}%, confidence={:.3f}%, sensitivity={:.3f}, robustness={:.3f}, step_sim={:.3f}\".format(best_arch, best_acc, best_conf, best_sensitivity, best_robust, best_step_sim))\n",
    "        target_cell.arch_cache = best_arch\n",
    "            \n",
    "best_archs = []\n",
    "for c in cells:\n",
    "    best_archs.append(c.arch_cache)\n",
    "    \n",
    "torch.save({\"model\":search_model.state_dict(), \"best_archs\":best_archs}, os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        logger.log(m.arch_cache)\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bde3bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(rank_confidences,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_sensitivities,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_robustnesses,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_step_sims,rank_accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da32ec",
   "metadata": {},
   "source": [
    "# Train a found model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ae460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch_nas_dataset=None, channel=16, config_path='./MY.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=66857, save_dir='./cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n",
      "Namespace(arch_nas_dataset=None, channel=16, config_path='./MY.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=66857, save_dir='./cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n"
     ]
    }
   ],
   "source": [
    "trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "print(args)\n",
    "args.save_dir = os.path.join(xargs.save_dir, \"train\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed27598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c02f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : None\n",
      "channel          : 16\n",
      "config_path      : ./MY.config\n",
      "data_path        : ../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 66857\n",
      "save_dir         : ./cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "Python  Version  : 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0]\n",
      "Pillow  Version  : 9.0.1\n",
      "PyTorch Version  : 1.12.0\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "../configs/nas-benchmark/CIFAR.config\n",
      "Configure(scheduler='cos', eta_min=0.0, epochs=200, warmup=0, optim='SGD', LR=0.1, decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=256, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Train-Loader-Num=196, Test-Loader-Num=40, batch size=256\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', eta_min=0.0, epochs=200, warmup=0, optim='SGD', LR=0.1, decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=256, class_num=10, xshape=(1, 3, 32, 32))\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|none~0|nor_conv_3x3~1|none~2|)\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_1x1~1|none~2|)\n",
      "Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|none~1|+|skip_connect~0|none~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|none~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|nor_conv_3x3~0|avg_pool_3x3~1|nor_conv_1x1~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_3x3~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|nor_conv_1x1~0|avg_pool_3x3~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|avg_pool_3x3~0|none~1|+|nor_conv_3x3~0|avg_pool_3x3~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|nor_conv_1x1~0|skip_connect~1|+|nor_conv_3x3~0|avg_pool_3x3~1|none~2|)\n",
      "Structure(4 nodes with |skip_connect~0|+|skip_connect~0|nor_conv_3x3~1|+|none~0|avg_pool_3x3~1|avg_pool_3x3~2|)\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|none~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_1x1~2|)\n",
      "Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_1x1~0|none~1|+|avg_pool_3x3~0|nor_conv_3x3~1|avg_pool_3x3~2|)\n",
      "Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|avg_pool_3x3~0|skip_connect~1|nor_conv_1x1~2|)\n",
      "Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_1x1~0|nor_conv_3x3~1|skip_connect~2|)\n",
      "Structure(4 nodes with |avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|none~0|skip_connect~1|nor_conv_3x3~2|)\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.1\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=200, current::epoch=0, iter=0.00, type=cosine, T-max=200, eta-min=0.0)\n",
      "criterion   : CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "logger = prepare_logger(args)\n",
    "\n",
    "# cifar_train_config_path = \"./MY.config\"\n",
    "cifar_train_config_path = \"../configs/nas-benchmark/CIFAR.config\"\n",
    "###\n",
    "train_data, test_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(cifar_train_config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=xargs.workers,\n",
    "            pin_memory=True,)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=xargs.workers,\n",
    "            pin_memory=True,)\n",
    "\n",
    "# search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "#                                                         valid_data,\n",
    "#                                                         xargs.dataset,\n",
    "#                                                         \"../configs/nas-benchmark/\",\n",
    "#                                                         (config.batch_size, config.batch_size),\n",
    "#                                                         xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Train-Loader-Num={:}, Test-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(train_loader), len(test_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": True, # true for eval\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "### load\n",
    "# trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "# search_model.load_state_dict(trained_output['model'], strict=False)\n",
    "best_archs = trained_output['best_archs']\n",
    "i=0\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        m.arch_cache = best_archs[i]\n",
    "        i += 1\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        print(m.arch_cache)\n",
    "###\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "\n",
    "start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "964fcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "#     data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "#     base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "#     network.train()\n",
    "#     end = time.time()\n",
    "#     for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(\n",
    "#         xloader\n",
    "#     ):\n",
    "#         scheduler.update(None, 1.0 * step / len(xloader))\n",
    "#         base_targets = base_targets.cuda(non_blocking=True)\n",
    "#         arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "#         # measure data loading time\n",
    "#         data_time.update(time.time() - end)\n",
    "\n",
    "#         w_optimizer.zero_grad()\n",
    "#         _, logits = network(base_inputs)\n",
    "#         base_loss = criterion(logits, base_targets)\n",
    "#         base_loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "#         w_optimizer.step()\n",
    "#         # record\n",
    "#         base_prec1, base_prec5 = obtain_accuracy(\n",
    "#             logits.data, base_targets.data, topk=(1, 5)\n",
    "#         )\n",
    "#         base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "#         base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "#         base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "#         # measure elapsed time\n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "\n",
    "#         if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "#             Sstr = (\n",
    "#                 \"*SEARCH* \"\n",
    "#                 + time_string()\n",
    "#                 + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "#             )\n",
    "#             Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "#                 batch_time=batch_time, data_time=data_time\n",
    "#             )\n",
    "#             Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "#                 loss=base_losses, top1=base_top1, top5=base_top5\n",
    "#             )\n",
    "#             logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "#     return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def train_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()\n",
    "    for step, (base_inputs, base_targets) in enumerate(\n",
    "        xloader\n",
    "    ):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "        base_targets = base_targets.cuda(non_blocking=True)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        _, logits = network(base_inputs)\n",
    "        base_loss = criterion(logits, base_targets)\n",
    "        base_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "        w_optimizer.step()\n",
    "        # record\n",
    "        base_prec1, base_prec5 = obtain_accuracy(\n",
    "            logits.data, base_targets.data, topk=(1, 5)\n",
    "        )\n",
    "        base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "        base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "        base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "            Sstr = (\n",
    "                \"*SEARCH* \"\n",
    "                + time_string()\n",
    "                + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "            )\n",
    "            Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "                batch_time=batch_time, data_time=data_time\n",
    "            )\n",
    "            Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "                loss=base_losses, top1=base_top1, top5=base_top5\n",
    "            )\n",
    "            logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "    return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def valid_func_one_arch(xloader, network, criterion):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, (arch_inputs, arch_targets) in enumerate(xloader):\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            # prediction\n",
    "\n",
    "#             network.module.random_genotype_per_cell(True)\n",
    "            _, logits = network(arch_inputs)\n",
    "            arch_loss = criterion(logits, arch_targets)\n",
    "            # record\n",
    "            arch_prec1, arch_prec5 = obtain_accuracy(\n",
    "                logits.data, arch_targets.data, topk=(1, 5)\n",
    "            )\n",
    "            arch_losses.update(arch_loss.item(), arch_inputs.size(0))\n",
    "            arch_top1.update(arch_prec1.item(), arch_inputs.size(0))\n",
    "            arch_top5.update(arch_prec5.item(), arch_inputs.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    return arch_losses.avg, arch_top1.avg, arch_top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be07a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Search the 000-200-th epoch] Time Left: [00:00:00], LR=0.1\n",
      "*SEARCH* [2022-11-03 08:36:47] [000-200][000/196] Time 0.54 (0.54) Data 0.28 (0.28) Base [Loss 2.370 (2.370)  Prec@1 8.59 (8.59) Prec@5 43.36 (43.36)]\n",
      "*SEARCH* [2022-11-03 08:37:19] [000-200][195/196] Time 0.16 (0.17) Data 0.00 (0.00) Base [Loss 1.276 (1.588)  Prec@1 56.25 (40.75) Prec@5 95.00 (89.23)]\n",
      "[000-200] searching : loss=1.59, accuracy@1=40.75%, accuracy@5=89.23%, time-cost=32.5 s\n",
      "[000-200] evaluate  : loss=1.96, accuracy@1=33.81%, accuracy@5=88.34%\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 000-200-th epoch : find the highest validation accuracy : 33.81%.\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 001-200-th epoch] Time Left: [01:59:00], LR=0.09999383162408304\n",
      "*SEARCH* [2022-11-03 08:37:23] [001-200][000/196] Time 0.37 (0.37) Data 0.25 (0.25) Base [Loss 1.515 (1.515)  Prec@1 44.92 (44.92) Prec@5 90.23 (90.23)]\n",
      "*SEARCH* [2022-11-03 08:37:55] [001-200][195/196] Time 0.11 (0.17) Data 0.00 (0.00) Base [Loss 0.956 (1.169)  Prec@1 67.50 (57.94) Prec@5 96.25 (95.45)]\n",
      "[001-200] searching : loss=1.17, accuracy@1=57.94%, accuracy@5=95.45%, time-cost=64.9 s\n",
      "[001-200] evaluate  : loss=1.63, accuracy@1=48.83%, accuracy@5=93.00%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 001-200-th epoch : find the highest validation accuracy : 48.83%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 002-200-th epoch] Time Left: [01:59:22], LR=0.09997532801828658\n",
      "*SEARCH* [2022-11-03 08:37:59] [002-200][000/196] Time 0.43 (0.43) Data 0.24 (0.24) Base [Loss 1.070 (1.070)  Prec@1 60.55 (60.55) Prec@5 97.66 (97.66)]\n",
      "*SEARCH* [2022-11-03 08:38:31] [002-200][195/196] Time 0.11 (0.16) Data 0.00 (0.00) Base [Loss 0.907 (1.008)  Prec@1 70.00 (63.75) Prec@5 96.25 (96.83)]\n",
      "[002-200] searching : loss=1.01, accuracy@1=63.75%, accuracy@5=96.83%, time-cost=97.2 s\n",
      "[002-200] evaluate  : loss=1.75, accuracy@1=49.02%, accuracy@5=92.76%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 002-200-th epoch : find the highest validation accuracy : 49.02%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 003-200-th epoch] Time Left: [01:58:59], LR=0.09994449374809851\n",
      "*SEARCH* [2022-11-03 08:38:35] [003-200][000/196] Time 0.43 (0.43) Data 0.26 (0.26) Base [Loss 0.974 (0.974)  Prec@1 66.41 (66.41) Prec@5 95.70 (95.70)]\n",
      "*SEARCH* [2022-11-03 08:39:06] [003-200][195/196] Time 0.18 (0.16) Data 0.00 (0.00) Base [Loss 0.903 (0.906)  Prec@1 68.75 (68.00) Prec@5 96.25 (97.42)]\n",
      "[003-200] searching : loss=0.91, accuracy@1=68.00%, accuracy@5=97.42%, time-cost=128.8 s\n",
      "[003-200] evaluate  : loss=1.58, accuracy@1=53.07%, accuracy@5=95.38%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 003-200-th epoch : find the highest validation accuracy : 53.07%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 004-200-th epoch] Time Left: [01:57:21], LR=0.09990133642141358\n",
      "*SEARCH* [2022-11-03 08:39:11] [004-200][000/196] Time 0.37 (0.37) Data 0.20 (0.20) Base [Loss 0.968 (0.968)  Prec@1 66.02 (66.02) Prec@5 98.44 (98.44)]\n",
      "*SEARCH* [2022-11-03 08:39:44] [004-200][195/196] Time 0.16 (0.17) Data 0.00 (0.00) Base [Loss 0.822 (0.823)  Prec@1 72.50 (70.77) Prec@5 95.00 (97.98)]\n",
      "[004-200] searching : loss=0.82, accuracy@1=70.77%, accuracy@5=97.98%, time-cost=161.8 s\n",
      "[004-200] evaluate  : loss=1.29, accuracy@1=59.36%, accuracy@5=94.48%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 004-200-th epoch : find the highest validation accuracy : 59.36%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 005-200-th epoch] Time Left: [02:01:23], LR=0.0998458666866564\n",
      "*SEARCH* [2022-11-03 08:39:48] [005-200][000/196] Time 0.39 (0.39) Data 0.27 (0.27) Base [Loss 0.894 (0.894)  Prec@1 66.02 (66.02) Prec@5 98.05 (98.05)]\n",
      "*SEARCH* [2022-11-03 08:40:20] [005-200][195/196] Time 0.26 (0.16) Data 0.00 (0.00) Base [Loss 0.892 (0.768)  Prec@1 73.75 (72.85) Prec@5 97.50 (98.31)]\n",
      "[005-200] searching : loss=0.77, accuracy@1=72.85%, accuracy@5=98.31%, time-cost=193.8 s\n",
      "[005-200] evaluate  : loss=1.27, accuracy@1=58.12%, accuracy@5=95.73%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 006-200-th epoch] Time Left: [01:56:27], LR=0.099778098230154\n",
      "*SEARCH* [2022-11-03 08:40:24] [006-200][000/196] Time 0.32 (0.32) Data 0.20 (0.20) Base [Loss 0.784 (0.784)  Prec@1 71.88 (71.88) Prec@5 98.05 (98.05)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SEARCH* [2022-11-03 08:40:55] [006-200][195/196] Time 0.10 (0.16) Data 0.00 (0.00) Base [Loss 1.021 (0.719)  Prec@1 66.25 (74.94) Prec@5 97.50 (98.52)]\n",
      "[006-200] searching : loss=0.72, accuracy@1=74.94%, accuracy@5=98.52%, time-cost=225.2 s\n",
      "[006-200] evaluate  : loss=1.28, accuracy@1=59.87%, accuracy@5=96.57%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 006-200-th epoch : find the highest validation accuracy : 59.87%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 007-200-th epoch] Time Left: [01:55:40], LR=0.099698047772759\n",
      "*SEARCH* [2022-11-03 08:41:00] [007-200][000/196] Time 0.33 (0.33) Data 0.22 (0.22) Base [Loss 0.726 (0.726)  Prec@1 74.22 (74.22) Prec@5 98.83 (98.83)]\n",
      "*SEARCH* [2022-11-03 08:41:31] [007-200][195/196] Time 0.08 (0.16) Data 0.00 (0.00) Base [Loss 0.860 (0.682)  Prec@1 72.50 (76.26) Prec@5 100.00 (98.58)]\n",
      "[007-200] searching : loss=0.68, accuracy@1=76.26%, accuracy@5=98.58%, time-cost=255.8 s\n",
      "[007-200] evaluate  : loss=1.31, accuracy@1=59.21%, accuracy@5=96.86%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 008-200-th epoch] Time Left: [01:49:33], LR=0.0996057350657239\n",
      "*SEARCH* [2022-11-03 08:41:35] [008-200][000/196] Time 0.32 (0.32) Data 0.19 (0.19) Base [Loss 0.717 (0.717)  Prec@1 75.39 (75.39) Prec@5 98.83 (98.83)]\n",
      "*SEARCH* [2022-11-03 08:42:01] [008-200][195/196] Time 0.10 (0.13) Data 0.00 (0.00) Base [Loss 0.695 (0.653)  Prec@1 77.50 (77.27) Prec@5 97.50 (98.74)]\n",
      "[008-200] searching : loss=0.65, accuracy@1=77.27%, accuracy@5=98.74%, time-cost=282.3 s\n",
      "[008-200] evaluate  : loss=1.11, accuracy@1=66.81%, accuracy@5=96.88%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 008-200-th epoch : find the highest validation accuracy : 66.81%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 009-200-th epoch] Time Left: [01:37:29], LR=0.09950118288582788\n",
      "*SEARCH* [2022-11-03 08:42:05] [009-200][000/196] Time 0.34 (0.34) Data 0.21 (0.21) Base [Loss 0.610 (0.610)  Prec@1 79.69 (79.69) Prec@5 98.05 (98.05)]\n",
      "*SEARCH* [2022-11-03 08:42:35] [009-200][195/196] Time 0.07 (0.15) Data 0.00 (0.00) Base [Loss 0.910 (0.631)  Prec@1 67.50 (78.16) Prec@5 95.00 (98.76)]\n",
      "[009-200] searching : loss=0.63, accuracy@1=78.16%, accuracy@5=98.76%, time-cost=312.3 s\n",
      "[009-200] evaluate  : loss=1.00, accuracy@1=67.52%, accuracy@5=97.96%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 009-200-th epoch : find the highest validation accuracy : 67.52%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 010-200-th epoch] Time Left: [01:46:35], LR=0.0993844170297569\n",
      "*SEARCH* [2022-11-03 08:42:39] [010-200][000/196] Time 0.33 (0.33) Data 0.21 (0.21) Base [Loss 0.638 (0.638)  Prec@1 78.12 (78.12) Prec@5 98.83 (98.83)]\n",
      "*SEARCH* [2022-11-03 08:43:08] [010-200][195/196] Time 0.10 (0.15) Data 0.00 (0.00) Base [Loss 0.583 (0.610)  Prec@1 76.25 (78.96) Prec@5 98.75 (98.92)]\n",
      "[010-200] searching : loss=0.61, accuracy@1=78.96%, accuracy@5=98.92%, time-cost=341.9 s\n",
      "[010-200] evaluate  : loss=1.18, accuracy@1=63.38%, accuracy@5=96.09%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 011-200-th epoch] Time Left: [01:43:54], LR=0.0992554663077387\n",
      "*SEARCH* [2022-11-03 08:43:12] [011-200][000/196] Time 0.37 (0.37) Data 0.23 (0.23) Base [Loss 0.613 (0.613)  Prec@1 79.69 (79.69) Prec@5 99.22 (99.22)]\n",
      "*SEARCH* [2022-11-03 08:43:43] [011-200][195/196] Time 0.09 (0.16) Data 0.00 (0.00) Base [Loss 0.556 (0.590)  Prec@1 80.00 (79.47) Prec@5 100.00 (99.01)]\n",
      "[011-200] searching : loss=0.59, accuracy@1=79.47%, accuracy@5=99.01%, time-cost=373.7 s\n",
      "[011-200] evaluate  : loss=1.18, accuracy@1=63.97%, accuracy@5=96.01%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 012-200-th epoch] Time Left: [01:52:10], LR=0.09911436253643445\n",
      "*SEARCH* [2022-11-03 08:43:48] [012-200][000/196] Time 0.45 (0.45) Data 0.20 (0.20) Base [Loss 0.609 (0.609)  Prec@1 82.03 (82.03) Prec@5 99.22 (99.22)]\n",
      "*SEARCH* [2022-11-03 08:44:18] [012-200][195/196] Time 0.16 (0.16) Data 0.00 (0.00) Base [Loss 0.427 (0.574)  Prec@1 85.00 (80.07) Prec@5 100.00 (99.00)]\n",
      "[012-200] searching : loss=0.57, accuracy@1=80.07%, accuracy@5=99.00%, time-cost=404.7 s\n",
      "[012-200] evaluate  : loss=0.97, accuracy@1=70.13%, accuracy@5=97.83%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 012-200-th epoch : find the highest validation accuracy : 70.13%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 013-200-th epoch] Time Left: [01:49:21], LR=0.09896114053108829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SEARCH* [2022-11-03 08:44:23] [013-200][000/196] Time 0.47 (0.47) Data 0.28 (0.28) Base [Loss 0.633 (0.633)  Prec@1 75.00 (75.00) Prec@5 98.83 (98.83)]\n",
      "*SEARCH* [2022-11-03 08:44:53] [013-200][195/196] Time 0.10 (0.16) Data 0.00 (0.00) Base [Loss 0.734 (0.561)  Prec@1 72.50 (80.51) Prec@5 96.25 (99.04)]\n",
      "[013-200] searching : loss=0.56, accuracy@1=80.51%, accuracy@5=99.04%, time-cost=435.5 s\n",
      "[013-200] evaluate  : loss=0.74, accuracy@1=75.60%, accuracy@5=98.27%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 013-200-th epoch : find the highest validation accuracy : 75.60%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 014-200-th epoch] Time Left: [01:46:14], LR=0.09879583809693737\n",
      "*SEARCH* [2022-11-03 08:44:57] [014-200][000/196] Time 0.35 (0.35) Data 0.20 (0.20) Base [Loss 0.645 (0.645)  Prec@1 81.25 (81.25) Prec@5 98.44 (98.44)]\n",
      "*SEARCH* [2022-11-03 08:45:29] [014-200][195/196] Time 0.17 (0.16) Data 0.00 (0.00) Base [Loss 0.713 (0.548)  Prec@1 73.75 (81.09) Prec@5 96.25 (99.10)]\n",
      "[014-200] searching : loss=0.55, accuracy@1=81.09%, accuracy@5=99.10%, time-cost=467.5 s\n",
      "[014-200] evaluate  : loss=0.71, accuracy@1=76.30%, accuracy@5=98.79%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 014-200-th epoch : find the highest validation accuracy : 76.30%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 015-200-th epoch] Time Left: [01:53:52], LR=0.09861849601988383\n",
      "*SEARCH* [2022-11-03 08:45:34] [015-200][000/196] Time 0.37 (0.37) Data 0.23 (0.23) Base [Loss 0.568 (0.568)  Prec@1 81.64 (81.64) Prec@5 99.61 (99.61)]\n",
      "*SEARCH* [2022-11-03 08:46:04] [015-200][195/196] Time 0.11 (0.15) Data 0.00 (0.00) Base [Loss 0.458 (0.532)  Prec@1 82.50 (81.72) Prec@5 98.75 (99.16)]\n",
      "[015-200] searching : loss=0.53, accuracy@1=81.72%, accuracy@5=99.16%, time-cost=497.6 s\n",
      "[015-200] evaluate  : loss=1.04, accuracy@1=68.04%, accuracy@5=97.69%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 016-200-th epoch] Time Left: [01:43:14], LR=0.09842915805643156\n",
      "*SEARCH* [2022-11-03 08:46:08] [016-200][000/196] Time 0.36 (0.36) Data 0.18 (0.18) Base [Loss 0.542 (0.542)  Prec@1 80.08 (80.08) Prec@5 99.22 (99.22)]\n",
      "*SEARCH* [2022-11-03 08:46:37] [016-200][195/196] Time 0.17 (0.15) Data 0.00 (0.00) Base [Loss 0.477 (0.530)  Prec@1 86.25 (81.66) Prec@5 100.00 (99.21)]\n",
      "[016-200] searching : loss=0.53, accuracy@1=81.66%, accuracy@5=99.21%, time-cost=527.7 s\n",
      "[016-200] evaluate  : loss=0.91, accuracy@1=71.02%, accuracy@5=98.19%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 017-200-th epoch] Time Left: [01:43:24], LR=0.09822787092288991\n",
      "*SEARCH* [2022-11-03 08:46:41] [017-200][000/196] Time 0.30 (0.30) Data 0.18 (0.18) Base [Loss 0.477 (0.477)  Prec@1 82.81 (82.81) Prec@5 98.83 (98.83)]\n",
      "*SEARCH* [2022-11-03 08:47:12] [017-200][195/196] Time 0.11 (0.16) Data 0.00 (0.00) Base [Loss 0.614 (0.520)  Prec@1 75.00 (81.94) Prec@5 98.75 (99.20)]\n",
      "[017-200] searching : loss=0.52, accuracy@1=81.94%, accuracy@5=99.20%, time-cost=558.5 s\n",
      "[017-200] evaluate  : loss=0.93, accuracy@1=71.97%, accuracy@5=97.90%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 018-200-th epoch] Time Left: [01:45:16], LR=0.09801468428384716\n",
      "*SEARCH* [2022-11-03 08:47:16] [018-200][000/196] Time 0.44 (0.44) Data 0.27 (0.27) Base [Loss 0.553 (0.553)  Prec@1 80.47 (80.47) Prec@5 99.61 (99.61)]\n",
      "*SEARCH* [2022-11-03 08:47:47] [018-200][195/196] Time 0.10 (0.16) Data 0.00 (0.00) Base [Loss 0.420 (0.511)  Prec@1 85.00 (82.34) Prec@5 100.00 (99.20)]\n",
      "[018-200] searching : loss=0.51, accuracy@1=82.34%, accuracy@5=99.20%, time-cost=589.3 s\n",
      "[018-200] evaluate  : loss=0.78, accuracy@1=74.41%, accuracy@5=98.26%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 019-200-th epoch] Time Left: [01:44:51], LR=0.09778965073991651\n",
      "*SEARCH* [2022-11-03 08:47:51] [019-200][000/196] Time 0.34 (0.34) Data 0.22 (0.22) Base [Loss 0.462 (0.462)  Prec@1 84.77 (84.77) Prec@5 99.22 (99.22)]\n",
      "*SEARCH* [2022-11-03 08:48:22] [019-200][195/196] Time 0.15 (0.16) Data 0.00 (0.00) Base [Loss 0.382 (0.499)  Prec@1 87.50 (82.77) Prec@5 98.75 (99.25)]\n",
      "[019-200] searching : loss=0.50, accuracy@1=82.77%, accuracy@5=99.25%, time-cost=621.0 s\n",
      "[019-200] evaluate  : loss=0.65, accuracy@1=78.16%, accuracy@5=98.80%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "<<<--->>> The 019-200-th epoch : find the highest validation accuracy : 78.16%.\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth exist, delete is at first before saving\n",
      "copy the file from cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-best.pth\n",
      "\n",
      "[Search the 020-200-th epoch] Time Left: [01:47:56], LR=0.09755282581475769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SEARCH* [2022-11-03 08:48:27] [020-200][000/196] Time 0.35 (0.35) Data 0.22 (0.22) Base [Loss 0.418 (0.418)  Prec@1 87.50 (87.50) Prec@5 99.22 (99.22)]\n",
      "*SEARCH* [2022-11-03 08:48:57] [020-200][195/196] Time 0.10 (0.15) Data 0.00 (0.00) Base [Loss 0.745 (0.490)  Prec@1 75.00 (83.13) Prec@5 98.75 (99.17)]\n",
      "[020-200] searching : loss=0.49, accuracy@1=83.13%, accuracy@5=99.17%, time-cost=651.2 s\n",
      "[020-200] evaluate  : loss=0.66, accuracy@1=77.74%, accuracy@5=98.93%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 021-200-th epoch] Time Left: [01:41:50], LR=0.09730426794137727\n",
      "*SEARCH* [2022-11-03 08:49:01] [021-200][000/196] Time 0.42 (0.42) Data 0.28 (0.28) Base [Loss 0.503 (0.503)  Prec@1 82.03 (82.03) Prec@5 99.61 (99.61)]\n",
      "*SEARCH* [2022-11-03 08:49:33] [021-200][195/196] Time 0.16 (0.16) Data 0.00 (0.00) Base [Loss 0.447 (0.485)  Prec@1 82.50 (83.16) Prec@5 100.00 (99.26)]\n",
      "[021-200] searching : loss=0.48, accuracy@1=83.16%, accuracy@5=99.26%, time-cost=683.4 s\n",
      "[021-200] evaluate  : loss=0.72, accuracy@1=77.26%, accuracy@5=98.66%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 022-200-th epoch] Time Left: [01:49:22], LR=0.09704403844771128\n",
      "*SEARCH* [2022-11-03 08:49:38] [022-200][000/196] Time 0.32 (0.32) Data 0.22 (0.22) Base [Loss 0.449 (0.449)  Prec@1 84.77 (84.77) Prec@5 99.22 (99.22)]\n",
      "*SEARCH* [2022-11-03 08:50:09] [022-200][195/196] Time 0.12 (0.16) Data 0.00 (0.00) Base [Loss 0.542 (0.483)  Prec@1 78.75 (83.35) Prec@5 100.00 (99.30)]\n",
      "[022-200] searching : loss=0.48, accuracy@1=83.35%, accuracy@5=99.30%, time-cost=714.8 s\n",
      "[022-200] evaluate  : loss=0.98, accuracy@1=71.59%, accuracy@5=97.71%\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/checkpoint/seed-66857-basic.pth\n",
      "Find cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into cell_level-arch_loop-reset_cell_params-loop1_ep10_sample200/train/seed-66857-last-info.pth\n",
      "\n",
      "[Search the 023-200-th epoch] Time Left: [01:43:58], LR=0.09677220154149338\n",
      "*SEARCH* [2022-11-03 08:50:13] [023-200][000/196] Time 0.34 (0.34) Data 0.23 (0.23) Base [Loss 0.446 (0.446)  Prec@1 84.77 (84.77) Prec@5 99.61 (99.61)]\n"
     ]
    }
   ],
   "source": [
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "for epoch in range(0, total_epoch):\n",
    "    w_scheduler.update(epoch, 0.0)\n",
    "    need_time = \"Time Left: {:}\".format(\n",
    "        convert_secs2time(epoch_time.val * (total_epoch - epoch), True)\n",
    "    )\n",
    "    epoch_str = \"{:03d}-{:03d}\".format(epoch, total_epoch)\n",
    "    logger.log(\n",
    "        \"\\n[Search the {:}-th epoch] {:}, LR={:}\".format(\n",
    "            epoch_str, need_time, min(w_scheduler.get_lr())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # selected_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    search_w_loss, search_w_top1, search_w_top5 = train_func_one_arch(\n",
    "        train_loader,\n",
    "        network,\n",
    "        criterion,\n",
    "        w_scheduler,\n",
    "        w_optimizer,\n",
    "        epoch_str,\n",
    "        xargs.print_freq,\n",
    "        logger,\n",
    "    )\n",
    "    search_time.update(time.time() - start_time)\n",
    "    logger.log(\n",
    "        \"[{:}] searching : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%, time-cost={:.1f} s\".format(\n",
    "            epoch_str, search_w_loss, search_w_top1, search_w_top5, search_time.sum\n",
    "        )\n",
    "    )\n",
    "    valid_a_loss, valid_a_top1, valid_a_top5 = valid_func_one_arch(\n",
    "        test_loader, network, criterion\n",
    "    )\n",
    "    logger.log(\n",
    "        \"[{:}] evaluate  : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(\n",
    "            epoch_str, valid_a_loss, valid_a_top1, valid_a_top5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # check the best accuracy\n",
    "    valid_accuracies[epoch] = valid_a_top1\n",
    "    if valid_a_top1 > valid_accuracies[\"best\"]:\n",
    "        valid_accuracies[\"best\"] = valid_a_top1\n",
    "        find_best = True\n",
    "    else:\n",
    "        find_best = False\n",
    "\n",
    "    # save checkpoint\n",
    "    save_path = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(xargs),\n",
    "            \"search_model\": search_model.state_dict(),\n",
    "            \"w_optimizer\": w_optimizer.state_dict(),\n",
    "            \"w_scheduler\": w_scheduler.state_dict(),\n",
    "            \"genotypes\": genotypes,\n",
    "            \"valid_accuracies\": valid_accuracies,\n",
    "        },\n",
    "        model_base_path,\n",
    "        logger,\n",
    "    )\n",
    "    last_info = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(args),\n",
    "            \"last_checkpoint\": save_path,\n",
    "        },\n",
    "        logger.path(\"info\"),\n",
    "        logger,\n",
    "    )\n",
    "    if find_best:\n",
    "        logger.log(\n",
    "            \"<<<--->>> The {:}-th epoch : find the highest validation accuracy : {:.2f}%.\".format(\n",
    "                epoch_str, valid_a_top1\n",
    "            )\n",
    "        )\n",
    "        copy_checkpoint(model_base_path, model_best_path, logger)\n",
    "    if api is not None:\n",
    "        logger.log(\"{:}\".format(api.query_by_arch(genotypes[epoch], \"200\")))\n",
    "    # measure elapsed time\n",
    "    epoch_time.update(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d00afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_archs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
