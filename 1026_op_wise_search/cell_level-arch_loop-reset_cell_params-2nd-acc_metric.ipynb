{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902bb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 08:45:33.050527: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "from custom_models import get_cell_based_tiny_net\n",
    "from custom_search_cells import NAS201SearchCell as SearchCell\n",
    "from xautodl.models.cell_searchs.genotypes import Structure\n",
    "\n",
    "# NB201\n",
    "from nas_201_api import NASBench201API as API\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792763c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56946\n",
      "Namespace(arch_nas_dataset=None, channel=16, config_path='./MY.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=56946, save_dir='./cell_level-arch_loop-no_reset_cell_params-loop3_ep3_sample200-acc_metric', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='../cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space_name\", type=str, default='nas-bench-201', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./MY.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "# log\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./cell_level-arch_loop-no_reset_cell_params-loop3_ep3_sample200-acc_metric', help=\"Folder to save checkpoints and log.\")\n",
    "# parser.add_argument(\"--arch_nas_dataset\", type=str, default='../NAS-Bench-201-v1_1-096897.pth', help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--arch_nas_dataset\", type=str, default=None, help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "    \n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcd1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=cell_level-arch_loop-no_reset_cell_params-loop3_ep3_sample200-acc_metric, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : None\n",
      "channel          : 16\n",
      "config_path      : ./MY.config\n",
      "data_path        : ../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 56946\n",
      "save_dir         : ./cell_level-arch_loop-no_reset_cell_params-loop3_ep3_sample200-acc_metric\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "Python  Version  : 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0]\n",
      "Pillow  Version  : 9.0.1\n",
      "PyTorch Version  : 1.12.0\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9daa5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./MY.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=50, current::epoch=0, iter=0.00, type=cosine, T-max=50, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "[2022-11-04 08:45:35] create API = None done\n",
      "=> do not find the last-info file : cell_level-arch_loop-no_reset_cell_params-loop3_ep3_sample200-acc_metric/seed-56946-last-info.pth\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.test_batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": bool(xargs.track_running_stats),\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "# if xargs.arch_nas_dataset is None:\n",
    "api = None\n",
    "# else:\n",
    "#     api = API(xargs.arch_nas_dataset)\n",
    "logger.log(\"{:} create API = {:} done\".format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "if last_info.exists():  # automatically resume from previous checkpoint\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start\".format(last_info)\n",
    "    )\n",
    "    last_info = torch.load(last_info)\n",
    "    start_epoch = last_info[\"epoch\"]\n",
    "    checkpoint = torch.load(last_info[\"last_checkpoint\"])\n",
    "    genotypes = checkpoint[\"genotypes\"]\n",
    "    valid_accuracies = checkpoint[\"valid_accuracies\"]\n",
    "    search_model.load_state_dict(checkpoint[\"search_model\"])\n",
    "    w_scheduler.load_state_dict(checkpoint[\"w_scheduler\"])\n",
    "    w_optimizer.load_state_dict(checkpoint[\"w_optimizer\"])\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(\n",
    "            last_info, start_epoch\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ba0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_confidence_robustness_metrics(network, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        # accuracy\n",
    "        network.train()\n",
    "        _, logits = network(inputs)\n",
    "        val_top1, val_top5 = obtain_accuracy(logits.data, targets.data, topk=(1, 5))\n",
    "        acc = val_top1\n",
    "        \n",
    "        return acc.item()\n",
    "        \n",
    "#         # confidence\n",
    "#         prob = torch.nn.functional.softmax(logits, dim=1)\n",
    "#         one_hot_idx = torch.nn.functional.one_hot(targets)\n",
    "#         confidence = (prob[one_hot_idx==1].sum()) / inputs.size(0) * 100 # in percent\n",
    "        \n",
    "#         # sensitivity\n",
    "#         _, noisy_logits = network(inputs + torch.randn_like(inputs)*0.1)\n",
    "#         kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#         sensitivity = kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        \n",
    "#         # robustness\n",
    "#         original_weights = deepcopy(network.state_dict())\n",
    "#         for m in network.modules():\n",
    "#             if isinstance(m, SearchCell):\n",
    "#                 for p in m.parameters():\n",
    "#                     p.add_(torch.randn_like(p) * p.std()*0.3)\n",
    "            \n",
    "#         _, noisy_logits = network(inputs)\n",
    "#         kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#         robustness = -kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "#         network.load_state_dict(original_weights)\n",
    "                \n",
    "#         return acc.item(), confidence.item(), sensitivity.item(), robustness.item()\n",
    "    \n",
    "def step_sim_metric(network, criterion, inputs, targets):\n",
    "#     inputs, targets = inputs[:64], targets[:64] # smaller batches\n",
    "    original_dict = deepcopy(network.state_dict())\n",
    "    optim_large_step = torch.optim.SGD(network.parameters(), lr=0.025)\n",
    "    \n",
    "    # single large step\n",
    "    network.train()\n",
    "    optim_large_step.zero_grad()\n",
    "    _, logits = network(inputs)\n",
    "    base_loss = criterion(logits, targets)\n",
    "    base_loss.backward()\n",
    "    optim_large_step.step()\n",
    "    large_step_dict = deepcopy(network.state_dict())\n",
    "    \n",
    "    # multiple small steps\n",
    "    network.load_state_dict(original_dict)\n",
    "    optim_small_step = torch.optim.SGD(network.parameters(), lr=0.025/3)\n",
    "    for i in range(3):\n",
    "        optim_small_step.zero_grad()\n",
    "        _, logits = network(inputs)\n",
    "        base_loss = criterion(logits, targets)\n",
    "        base_loss.backward()\n",
    "        optim_small_step.step()\n",
    "    small_step_dict = deepcopy(network.state_dict())\n",
    "    scores = []\n",
    "    for key in large_step_dict.keys():\n",
    "        if ('weight' in key) and (original_dict[key].dim()==4):\n",
    "            if (original_dict[key] != large_step_dict[key]).sum():\n",
    "                large_step = large_step_dict[key] - original_dict[key]\n",
    "                small_step = small_step_dict[key] - original_dict[key]\n",
    "                co, ci, kh, kw = large_step.size()\n",
    "                large_step = large_step.view(co, -1)\n",
    "                small_step = small_step.view(co, -1)\n",
    "                score = torch.nn.functional.cosine_similarity(large_step, small_step, dim=1)\n",
    "                score = score.mean().item() * 100 # in percent\n",
    "                scores.append(score)\n",
    "    if len(scores)==0:\n",
    "        step_sim = 100\n",
    "        raise RuntimeError\n",
    "    else:\n",
    "        step_sim = np.mean(scores)\n",
    "    \n",
    "    # resume\n",
    "    network.load_state_dict(original_dict)\n",
    "            \n",
    "    return step_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdab22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of nodes:60\n",
      "target_node:1 4\n",
      "target_node:2 24\n",
      "target_node:3 124\n",
      "\n",
      "\n",
      " Searching with a cell #0\n",
      "*Train* [2022-11-04 08:45:39] Ep:0 [000/391] Time 2.96 (2.96) Data 0.18 (0.18) Base [Loss 2.379 (2.379)  Prec@1 7.81 (7.81) Prec@5 37.50 (37.50)]\n",
      "*Train* [2022-11-04 08:46:03] Ep:0 [200/391] Time 0.08 (0.13) Data 0.00 (0.00) Base [Loss 1.598 (1.826)  Prec@1 34.38 (32.38) Prec@5 89.06 (84.03)]\n",
      "*Train* [2022-11-04 08:46:25] Ep:0 [390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 1.440 (1.687)  Prec@1 57.50 (37.56) Prec@5 90.00 (87.55)]\n",
      "Ep:0 ends : loss=1.69, accuracy@1=37.56%, accuracy@5=87.55%\n",
      "*Train* [2022-11-04 08:46:25] Ep:1 [000/391] Time 0.35 (0.35) Data 0.20 (0.20) Base [Loss 2.770 (2.770)  Prec@1 14.06 (14.06) Prec@5 62.50 (62.50)]\n",
      "*Train* [2022-11-04 08:46:47] Ep:1 [200/391] Time 0.21 (0.11) Data 0.00 (0.00) Base [Loss 1.491 (1.525)  Prec@1 54.69 (44.16) Prec@5 90.62 (90.96)]\n",
      "*Train* [2022-11-04 08:47:10] Ep:1 [390/391] Time 0.06 (0.12) Data 0.00 (0.00) Base [Loss 1.411 (1.440)  Prec@1 50.00 (47.73) Prec@5 90.00 (92.07)]\n",
      "Ep:1 ends : loss=1.44, accuracy@1=47.73%, accuracy@5=92.07%\n",
      "*Train* [2022-11-04 08:47:10] Ep:2 [000/391] Time 0.26 (0.26) Data 0.18 (0.18) Base [Loss 2.824 (2.824)  Prec@1 10.94 (10.94) Prec@5 51.56 (51.56)]\n",
      "*Train* [2022-11-04 08:47:31] Ep:2 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 1.206 (1.375)  Prec@1 53.12 (50.63) Prec@5 92.19 (93.07)]\n",
      "*Train* [2022-11-04 08:47:55] Ep:2 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 1.156 (1.300)  Prec@1 55.00 (53.49) Prec@5 92.50 (93.90)]\n",
      "Ep:2 ends : loss=1.30, accuracy@1=53.49%, accuracy@5=93.90%\n",
      "Found best op for target cell:0\n",
      ": Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_3x3~0|avg_pool_3x3~1|+|avg_pool_3x3~0|nor_conv_3x3~1|nor_conv_1x1~2|) with accuracy=48.05%\n",
      "\n",
      "\n",
      " Searching with a cell #1\n",
      "*Train* [2022-11-04 08:48:11] Ep:0 [000/391] Time 0.32 (0.32) Data 0.22 (0.22) Base [Loss 2.977 (2.977)  Prec@1 14.06 (14.06) Prec@5 48.44 (48.44)]\n",
      "*Train* [2022-11-04 08:48:33] Ep:0 [200/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 1.379 (1.293)  Prec@1 56.25 (54.17) Prec@5 90.62 (93.78)]\n",
      "*Train* [2022-11-04 08:48:50] Ep:0 [390/391] Time 0.08 (0.10) Data 0.00 (0.00) Base [Loss 0.915 (1.220)  Prec@1 62.50 (56.46) Prec@5 97.50 (94.74)]\n",
      "Ep:0 ends : loss=1.22, accuracy@1=56.46%, accuracy@5=94.74%\n",
      "*Train* [2022-11-04 08:48:50] Ep:1 [000/391] Time 0.34 (0.34) Data 0.19 (0.19) Base [Loss 3.061 (3.061)  Prec@1 14.06 (14.06) Prec@5 54.69 (54.69)]\n",
      "*Train* [2022-11-04 08:49:11] Ep:1 [200/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 1.241 (1.205)  Prec@1 53.12 (57.17) Prec@5 96.88 (94.82)]\n",
      "*Train* [2022-11-04 08:49:31] Ep:1 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.996 (1.155)  Prec@1 62.50 (59.01) Prec@5 97.50 (95.36)]\n",
      "Ep:1 ends : loss=1.15, accuracy@1=59.01%, accuracy@5=95.36%\n",
      "*Train* [2022-11-04 08:49:32] Ep:2 [000/391] Time 0.28 (0.28) Data 0.20 (0.20) Base [Loss 2.292 (2.292)  Prec@1 17.19 (17.19) Prec@5 76.56 (76.56)]\n",
      "*Train* [2022-11-04 08:49:50] Ep:2 [200/391] Time 0.12 (0.09) Data 0.00 (0.00) Base [Loss 0.853 (1.115)  Prec@1 71.88 (60.55) Prec@5 95.31 (95.70)]\n",
      "*Train* [2022-11-04 08:50:14] Ep:2 [390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 0.982 (1.084)  Prec@1 70.00 (61.48) Prec@5 92.50 (95.95)]\n",
      "Ep:2 ends : loss=1.08, accuracy@1=61.48%, accuracy@5=95.95%\n",
      "Found best op for target cell:1\n",
      ": Structure(4 nodes with |skip_connect~0|+|none~0|avg_pool_3x3~1|+|avg_pool_3x3~0|none~1|nor_conv_1x1~2|) with accuracy=61.33%\n",
      "\n",
      "\n",
      " Searching with a cell #2\n",
      "*Train* [2022-11-04 08:50:32] Ep:0 [000/391] Time 0.39 (0.39) Data 0.26 (0.26) Base [Loss 1.237 (1.237)  Prec@1 56.25 (56.25) Prec@5 92.19 (92.19)]\n",
      "*Train* [2022-11-04 08:50:55] Ep:0 [200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 1.124 (1.036)  Prec@1 60.94 (62.80) Prec@5 93.75 (96.53)]\n",
      "*Train* [2022-11-04 08:51:13] Ep:0 [390/391] Time 0.08 (0.10) Data 0.00 (0.00) Base [Loss 0.903 (1.022)  Prec@1 60.00 (63.46) Prec@5 97.50 (96.69)]\n",
      "Ep:0 ends : loss=1.02, accuracy@1=63.46%, accuracy@5=96.69%\n",
      "*Train* [2022-11-04 08:51:13] Ep:1 [000/391] Time 0.27 (0.27) Data 0.17 (0.17) Base [Loss 2.051 (2.051)  Prec@1 29.69 (29.69) Prec@5 84.38 (84.38)]\n",
      "*Train* [2022-11-04 08:51:36] Ep:1 [200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 1.088 (1.019)  Prec@1 65.62 (63.92) Prec@5 92.19 (96.40)]\n",
      "*Train* [2022-11-04 08:51:56] Ep:1 [390/391] Time 0.08 (0.11) Data 0.00 (0.00) Base [Loss 0.801 (0.997)  Prec@1 72.50 (64.76) Prec@5 100.00 (96.62)]\n",
      "Ep:1 ends : loss=1.00, accuracy@1=64.76%, accuracy@5=96.62%\n",
      "*Train* [2022-11-04 08:51:57] Ep:2 [000/391] Time 0.29 (0.29) Data 0.20 (0.20) Base [Loss 1.204 (1.204)  Prec@1 56.25 (56.25) Prec@5 93.75 (93.75)]\n",
      "*Train* [2022-11-04 08:52:18] Ep:2 [200/391] Time 0.12 (0.11) Data 0.00 (0.00) Base [Loss 0.934 (0.957)  Prec@1 67.19 (66.18) Prec@5 98.44 (97.01)]\n",
      "*Train* [2022-11-04 08:52:35] Ep:2 [390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 1.170 (0.950)  Prec@1 65.00 (66.35) Prec@5 92.50 (97.04)]\n",
      "Ep:2 ends : loss=0.95, accuracy@1=66.35%, accuracy@5=97.04%\n",
      "Found best op for target cell:2\n",
      ": Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|avg_pool_3x3~1|+|skip_connect~0|none~1|none~2|) with accuracy=66.41%\n",
      "\n",
      "\n",
      " Searching with a cell #3\n",
      "*Train* [2022-11-04 08:52:52] Ep:0 [000/391] Time 0.34 (0.34) Data 0.23 (0.23) Base [Loss 0.938 (0.938)  Prec@1 70.31 (70.31) Prec@5 98.44 (98.44)]\n",
      "*Train* [2022-11-04 08:53:08] Ep:0 [200/391] Time 0.07 (0.08) Data 0.00 (0.00) Base [Loss 0.829 (0.916)  Prec@1 73.44 (68.16) Prec@5 93.75 (97.05)]\n",
      "*Train* [2022-11-04 08:53:24] Ep:0 [390/391] Time 0.08 (0.08) Data 0.00 (0.00) Base [Loss 0.998 (0.917)  Prec@1 60.00 (67.77) Prec@5 100.00 (97.17)]\n",
      "Ep:0 ends : loss=0.92, accuracy@1=67.77%, accuracy@5=97.17%\n",
      "*Train* [2022-11-04 08:53:25] Ep:1 [000/391] Time 0.28 (0.28) Data 0.18 (0.18) Base [Loss 1.602 (1.602)  Prec@1 42.19 (42.19) Prec@5 89.06 (89.06)]\n",
      "*Train* [2022-11-04 08:53:42] Ep:1 [200/391] Time 0.07 (0.09) Data 0.00 (0.00) Base [Loss 0.950 (0.929)  Prec@1 73.44 (66.82) Prec@5 93.75 (96.98)]\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "\n",
    "################# initialize\n",
    "cells = []\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        cells.append(m)\n",
    "num_cells = len(cells)\n",
    "print(\"total number of nodes:{}\".format(num_cells*xargs.max_nodes))\n",
    "        \n",
    "op_names = deepcopy(cells[0].op_names)\n",
    "op_names_wo_none = deepcopy(op_names)\n",
    "if \"none\" in op_names_wo_none:\n",
    "    op_names_wo_none.remove(\"none\")\n",
    "\n",
    "genotypes = []\n",
    "for i in range(1, xargs.max_nodes):\n",
    "    xlist = []\n",
    "    for j in range(i):\n",
    "        node_str = \"{:}<-{:}\".format(i, j)\n",
    "        if i-j==1:\n",
    "            op_name = \"skip_connect\"\n",
    "        else:\n",
    "            op_name = \"none\"\n",
    "        xlist.append((op_name, j))\n",
    "    genotypes.append(tuple(xlist))\n",
    "init_arch = Structure(genotypes)\n",
    "\n",
    "for c in cells:\n",
    "    c.arch_cache = init_arch\n",
    "\n",
    "### gen possible connections of a target node\n",
    "possible_connections = {}\n",
    "for target_node_idx in range(1,xargs.max_nodes):\n",
    "    possible_connections[target_node_idx] = list()\n",
    "    xlists = []\n",
    "    for src_node in range(target_node_idx):\n",
    "        node_str = \"{:}<-{:}\".format(target_node_idx, src_node)\n",
    "        # select possible ops\n",
    "#         if target_node_idx - src_node == 1:\n",
    "#             op_names_tmp = op_names_wo_none\n",
    "#         else:\n",
    "#             op_names_tmp = op_names\n",
    "        op_names_tmp = op_names\n",
    "            \n",
    "        if len(xlists) == 0: # initial iteration\n",
    "            for op_name in op_names_tmp:\n",
    "                xlists.append([(op_name, src_node)])\n",
    "        else:\n",
    "            new_xlists = []\n",
    "            for op_name in op_names_tmp:\n",
    "                for xlist in xlists:\n",
    "                    new_xlist = deepcopy(xlist)\n",
    "                    new_xlist.append((op_name, src_node))\n",
    "                    new_xlists.append(new_xlist)\n",
    "            xlists = new_xlists\n",
    "    for xlist in xlists:\n",
    "        selected_ops = []\n",
    "        for l in xlist:\n",
    "            selected_ops.append(l[0])\n",
    "        if sum(np.array(selected_ops) == \"none\") == len(selected_ops):\n",
    "            continue\n",
    "        possible_connections[target_node_idx].append(tuple(xlist))\n",
    "    print(\"target_node:{}\".format(target_node_idx), len(possible_connections[target_node_idx]))\n",
    "        \n",
    "### train while generating random architectures by mutating connections of a target node\n",
    "\n",
    "for arch_loop in range(3):\n",
    "    for target_cell_idx in range(num_cells):\n",
    "        target_cell = cells[target_cell_idx]\n",
    "        print(\"\\n\\n Searching with a cell #{}\".format(target_cell_idx))\n",
    "        ####\n",
    "#         for m in target_cell.modules():\n",
    "#             if hasattr(m, 'reset_parameters'):\n",
    "#                 m.reset_parameters()\n",
    "        ####\n",
    "        ## training\n",
    "        for ep in range(3):\n",
    "            ###\n",
    "            genotypes = []\n",
    "            for n in range(1, xargs.max_nodes):\n",
    "                genotypes.append(random.choice(possible_connections[n]))\n",
    "            arch = Structure(genotypes)\n",
    "            target_cell.arch_cache = arch\n",
    "#             arch = target_cell.random_genotype(True)\n",
    "            ###\n",
    "            data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "            base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "            network.train()\n",
    "            end = time.time()\n",
    "            print_freq = 200\n",
    "            for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(search_loader):\n",
    "                ######### forward/backward/optim\n",
    "                base_targets = base_targets.cuda(non_blocking=True)\n",
    "                arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "                # measure data loading time\n",
    "                data_time.update(time.time() - end)\n",
    "                w_optimizer.zero_grad()\n",
    "                _, logits = network(base_inputs)\n",
    "                base_loss = criterion(logits, base_targets)\n",
    "                base_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "                w_optimizer.step()\n",
    "\n",
    "                ######### logging\n",
    "                base_prec1, base_prec5 = obtain_accuracy(logits.data, base_targets.data, topk=(1, 5))\n",
    "                base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "                base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "                base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "                if step % print_freq == 0 or step + 1 == len(search_loader):\n",
    "                    Sstr = (\"*Train* \"+ time_string()+\" Ep:{:} [{:03d}/{:03d}]\".format(ep, step, len(search_loader)))\n",
    "                    Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(batch_time=batch_time, data_time=data_time)\n",
    "                    Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(loss=base_losses, top1=base_top1, top5=base_top5)\n",
    "                    logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "\n",
    "            logger.log(\"Ep:{:} ends : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(ep, base_losses.avg, base_top1.avg, base_top5.avg))\n",
    "        ## evaluation\n",
    "        network.train()\n",
    "        archs, metric_accs = [], []\n",
    "        loader_iter = iter(valid_loader)\n",
    "        for search_iter in range(200):\n",
    "            ###### random gen\n",
    "            genotypes = []\n",
    "            for n in range(1, xargs.max_nodes):\n",
    "                genotypes.append(random.choice(possible_connections[n]))\n",
    "            arch = Structure(genotypes)\n",
    "            target_cell.arch_cache = arch\n",
    "#             arch = target_cell.random_genotype(True)\n",
    "            ###### measure metrics\n",
    "            try:\n",
    "                inputs, targets = next(loader_iter)\n",
    "            except:\n",
    "                loader_iter = iter(valid_loader)\n",
    "                inputs, targets = next(loader_iter)\n",
    "            inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            valid_acc = acc_confidence_robustness_metrics(network, inputs, targets)\n",
    "            archs.append(arch)\n",
    "            metric_accs.append(valid_acc)\n",
    "        rank_accs = stats.rankdata(metric_accs)\n",
    "        rank_agg = rank_accs\n",
    "#         l = len(rank_accs)\n",
    "#         rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_robustnesses/l)+np.log(rank_step_sims/l)\n",
    "#             rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_step_sims/l)\n",
    "        best_idx = np.argmax(rank_agg)\n",
    "        best_arch, best_acc = archs[best_idx], metric_accs[best_idx]\n",
    "        logger.log(\"Found best op for target cell:{}\".format(target_cell_idx))\n",
    "        logger.log(\": {:} with accuracy={:.2f}%\".format(best_arch, best_acc))\n",
    "        target_cell.arch_cache = best_arch\n",
    "            \n",
    "best_archs = []\n",
    "for c in cells:\n",
    "    best_archs.append(c.arch_cache)\n",
    "    \n",
    "torch.save({\"model\":search_model.state_dict(), \"best_archs\":best_archs}, os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        logger.log(m.arch_cache)\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.scatter(rank_confidences,rank_accs)\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(rank_sensitivities,rank_accs)\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(rank_robustnesses,rank_accs)\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(rank_step_sims,rank_accs)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da32ec",
   "metadata": {},
   "source": [
    "# Train a found model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "print(args)\n",
    "args.save_dir = os.path.join(xargs.save_dir, \"train\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = prepare_logger(args)\n",
    "\n",
    "# cifar_train_config_path = \"./MY.config\"\n",
    "cifar_train_config_path = \"../configs/nas-benchmark/CIFAR.config\"\n",
    "###\n",
    "train_data, test_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(cifar_train_config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=xargs.workers,\n",
    "            pin_memory=True,)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=xargs.workers,\n",
    "            pin_memory=True,)\n",
    "\n",
    "# search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "#                                                         valid_data,\n",
    "#                                                         xargs.dataset,\n",
    "#                                                         \"../configs/nas-benchmark/\",\n",
    "#                                                         (config.batch_size, config.batch_size),\n",
    "#                                                         xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Train-Loader-Num={:}, Test-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(train_loader), len(test_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": True, # true for eval\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "### load\n",
    "# trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "# search_model.load_state_dict(trained_output['model'], strict=False)\n",
    "best_archs = trained_output['best_archs']\n",
    "i=0\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        m.arch_cache = best_archs[i]\n",
    "        i += 1\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        print(m.arch_cache)\n",
    "###\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "\n",
    "start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "#     data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "#     base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "#     network.train()\n",
    "#     end = time.time()\n",
    "#     for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(\n",
    "#         xloader\n",
    "#     ):\n",
    "#         scheduler.update(None, 1.0 * step / len(xloader))\n",
    "#         base_targets = base_targets.cuda(non_blocking=True)\n",
    "#         arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "#         # measure data loading time\n",
    "#         data_time.update(time.time() - end)\n",
    "\n",
    "#         w_optimizer.zero_grad()\n",
    "#         _, logits = network(base_inputs)\n",
    "#         base_loss = criterion(logits, base_targets)\n",
    "#         base_loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "#         w_optimizer.step()\n",
    "#         # record\n",
    "#         base_prec1, base_prec5 = obtain_accuracy(\n",
    "#             logits.data, base_targets.data, topk=(1, 5)\n",
    "#         )\n",
    "#         base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "#         base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "#         base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "#         # measure elapsed time\n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "\n",
    "#         if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "#             Sstr = (\n",
    "#                 \"*SEARCH* \"\n",
    "#                 + time_string()\n",
    "#                 + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "#             )\n",
    "#             Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "#                 batch_time=batch_time, data_time=data_time\n",
    "#             )\n",
    "#             Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "#                 loss=base_losses, top1=base_top1, top5=base_top5\n",
    "#             )\n",
    "#             logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "#     return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def train_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()\n",
    "    for step, (base_inputs, base_targets) in enumerate(\n",
    "        xloader\n",
    "    ):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "        base_targets = base_targets.cuda(non_blocking=True)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        _, logits = network(base_inputs)\n",
    "        base_loss = criterion(logits, base_targets)\n",
    "        base_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "        w_optimizer.step()\n",
    "        # record\n",
    "        base_prec1, base_prec5 = obtain_accuracy(\n",
    "            logits.data, base_targets.data, topk=(1, 5)\n",
    "        )\n",
    "        base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "        base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "        base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "            Sstr = (\n",
    "                \"*SEARCH* \"\n",
    "                + time_string()\n",
    "                + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "            )\n",
    "            Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "                batch_time=batch_time, data_time=data_time\n",
    "            )\n",
    "            Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "                loss=base_losses, top1=base_top1, top5=base_top5\n",
    "            )\n",
    "            logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "    return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def valid_func_one_arch(xloader, network, criterion):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, (arch_inputs, arch_targets) in enumerate(xloader):\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            # prediction\n",
    "\n",
    "#             network.module.random_genotype_per_cell(True)\n",
    "            _, logits = network(arch_inputs)\n",
    "            arch_loss = criterion(logits, arch_targets)\n",
    "            # record\n",
    "            arch_prec1, arch_prec5 = obtain_accuracy(\n",
    "                logits.data, arch_targets.data, topk=(1, 5)\n",
    "            )\n",
    "            arch_losses.update(arch_loss.item(), arch_inputs.size(0))\n",
    "            arch_top1.update(arch_prec1.item(), arch_inputs.size(0))\n",
    "            arch_top5.update(arch_prec5.item(), arch_inputs.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    return arch_losses.avg, arch_top1.avg, arch_top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be07a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "for epoch in range(0, total_epoch):\n",
    "    w_scheduler.update(epoch, 0.0)\n",
    "    need_time = \"Time Left: {:}\".format(\n",
    "        convert_secs2time(epoch_time.val * (total_epoch - epoch), True)\n",
    "    )\n",
    "    epoch_str = \"{:03d}-{:03d}\".format(epoch, total_epoch)\n",
    "    logger.log(\n",
    "        \"\\n[Search the {:}-th epoch] {:}, LR={:}\".format(\n",
    "            epoch_str, need_time, min(w_scheduler.get_lr())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # selected_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    search_w_loss, search_w_top1, search_w_top5 = train_func_one_arch(\n",
    "        train_loader,\n",
    "        network,\n",
    "        criterion,\n",
    "        w_scheduler,\n",
    "        w_optimizer,\n",
    "        epoch_str,\n",
    "        xargs.print_freq,\n",
    "        logger,\n",
    "    )\n",
    "    search_time.update(time.time() - start_time)\n",
    "    logger.log(\n",
    "        \"[{:}] searching : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%, time-cost={:.1f} s\".format(\n",
    "            epoch_str, search_w_loss, search_w_top1, search_w_top5, search_time.sum\n",
    "        )\n",
    "    )\n",
    "    valid_a_loss, valid_a_top1, valid_a_top5 = valid_func_one_arch(\n",
    "        test_loader, network, criterion\n",
    "    )\n",
    "    logger.log(\n",
    "        \"[{:}] evaluate  : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(\n",
    "            epoch_str, valid_a_loss, valid_a_top1, valid_a_top5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # check the best accuracy\n",
    "    valid_accuracies[epoch] = valid_a_top1\n",
    "    if valid_a_top1 > valid_accuracies[\"best\"]:\n",
    "        valid_accuracies[\"best\"] = valid_a_top1\n",
    "        find_best = True\n",
    "    else:\n",
    "        find_best = False\n",
    "\n",
    "    # save checkpoint\n",
    "    save_path = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(xargs),\n",
    "            \"search_model\": search_model.state_dict(),\n",
    "            \"w_optimizer\": w_optimizer.state_dict(),\n",
    "            \"w_scheduler\": w_scheduler.state_dict(),\n",
    "            \"genotypes\": genotypes,\n",
    "            \"valid_accuracies\": valid_accuracies,\n",
    "        },\n",
    "        model_base_path,\n",
    "        logger,\n",
    "    )\n",
    "    last_info = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(args),\n",
    "            \"last_checkpoint\": save_path,\n",
    "        },\n",
    "        logger.path(\"info\"),\n",
    "        logger,\n",
    "    )\n",
    "    if find_best:\n",
    "        logger.log(\n",
    "            \"<<<--->>> The {:}-th epoch : find the highest validation accuracy : {:.2f}%.\".format(\n",
    "                epoch_str, valid_a_top1\n",
    "            )\n",
    "        )\n",
    "        copy_checkpoint(model_base_path, model_best_path, logger)\n",
    "    if api is not None:\n",
    "        logger.log(\"{:}\".format(api.query_by_arch(genotypes[epoch], \"200\")))\n",
    "    # measure elapsed time\n",
    "    epoch_time.update(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d00afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_archs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
